{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "8e4e3308",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import GPT2Tokenizer\n",
    "\n",
    "tokenizer = GPT2Tokenizer.from_pretrained(\"gpt2\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "beef3eb8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[15496, 11, 12520, 234, 235, 0, 220, 19526, 254, 25001, 121, 0]\n"
     ]
    }
   ],
   "source": [
    "string = \"Hello, üåç! ‰Ω†Â•Ω!\"\n",
    "print(tokenizer.encode(string))\n",
    "tokens = tokenizer.encode(string)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "2ebdad81",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "15496 ->  Hello\n",
      "11 ->  ,\n",
      "12520 ->   ÔøΩ\n",
      "234 ->  ÔøΩ\n",
      "235 ->  ÔøΩ\n",
      "0 ->  !\n",
      "220 ->   \n",
      "19526 ->  ÔøΩ\n",
      "254 ->  ÔøΩ\n",
      "25001 ->  ÔøΩ\n",
      "121 ->  ÔøΩ\n",
      "0 ->  !\n"
     ]
    }
   ],
   "source": [
    "# decode single value\n",
    "for token in tokens:\n",
    "    print(f\"{token} -> \", tokenizer.decode(token))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "a8f78a7c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compression_ratio(string:str, tokens:list):\n",
    "    original_size = len(bytes(string, encoding=\"utf-8\"))\n",
    "    num_tokens = len(tokens)\n",
    "    return original_size/num_tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "c8b310fb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "original_size ->  20\n",
      "num_tokens ->  12\n",
      "compression ->  1.6666666666666667\n",
      "compression_formula ->  1.6666666666666667\n"
     ]
    }
   ],
   "source": [
    "# compression ratio\n",
    "original_size = len(bytes(string, encoding=\"utf-8\"))\n",
    "num_tokens = len(tokens)\n",
    "\n",
    "\n",
    "print(\"original_size -> \", original_size)\n",
    "print(\"num_tokens -> \", num_tokens)\n",
    "print(\"compression -> \", original_size/num_tokens)\n",
    "print(\"compression_formula -> \", compression_ratio(string=string, tokens=tokens))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "id": "6b68db19",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "H\n",
      "e\n",
      "l\n",
      "l\n",
      "o\n",
      ",\n",
      " \n",
      "üåç\n",
      "!\n",
      " \n",
      "‰Ω†\n",
      "Â•Ω\n",
      "!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "({'H': 0,\n",
       "  'e': 1,\n",
       "  'l': 2,\n",
       "  'o': 3,\n",
       "  ',': 4,\n",
       "  ' ': 5,\n",
       "  'üåç': 6,\n",
       "  '!': 7,\n",
       "  '‰Ω†': 8,\n",
       "  'Â•Ω': 9},\n",
       " [0, 1, 2, 2, 3, 4, 5, 6, 7, 5, 8, 9, 7])"
      ]
     },
     "execution_count": 75,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Options for tokenizes\n",
    "# Option 1: Each character a token\n",
    "# Lets then map each character to a number\n",
    "# do I need catalog? \n",
    "\n",
    "vocab_dict = {}\n",
    "index_cnt = 0\n",
    "char_tokens = []\n",
    "for char in string: \n",
    "    print(char)\n",
    "    if char not in vocab_dict:\n",
    "        vocab_dict[char] = index_cnt\n",
    "        char_tokens.append(index_cnt)\n",
    "        index_cnt += 1\n",
    "\n",
    "    else:\n",
    "        char_tokens.append(vocab_dict[char])\n",
    "\n",
    "#tokens\n",
    "vocab_dict, char_tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "b980ddff",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "char compression -> ,  1.5384615384615385\n"
     ]
    }
   ],
   "source": [
    "print(\"char compression -> , \", compression_ratio(string=string, tokens=char_tokens))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "14470bb2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "97"
      ]
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ord(\"a\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "af02d31b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'a'"
      ]
     },
     "execution_count": 69,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chr(97)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "5346ee6e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "b'a'\n",
      "b'\\xf0\\x9f\\x8c\\x8e'\n"
     ]
    }
   ],
   "source": [
    "# Byte-based tokenization\n",
    "# unicode strings can be represented as integers between 0 and 255\n",
    "# utf-8 representation\n",
    "print(bytes(\"a\", encoding=\"utf-8\"))\n",
    "print(bytes('üåé', encoding=\"utf-8\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "id": "96505f40",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "240"
      ]
     },
     "execution_count": 91,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "int.from_bytes(b'\\xf0')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "id": "ed30dcbf",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4"
      ]
     },
     "execution_count": 101,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(bytes('üåé', encoding=\"utf-8\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "id": "d86c432a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "byte tokens:  [72, 101, 108, 108, 111, 44, 32, 240, 159, 140, 141, 33, 32, 228, 189, 160, 229, 165, 189, 33]\n"
     ]
    }
   ],
   "source": [
    "def byte_char_encoding(string:str):\n",
    "    \"\"\" Splits string in separate characters\n",
    "     Split in single bytes, case a character is represented by more than 1 byte. \n",
    "       \"\"\"\n",
    "    vocab_dict = {}\n",
    "\n",
    "    char_tokens = []\n",
    "    for char in string: \n",
    "        #print(bytes(char, encoding=\"utf-8\"))\n",
    "        char_bytes = bytes(char, encoding=\"utf-8\")\n",
    "        if len(char_bytes) == 0:\n",
    "            byte = char_bytes[0]\n",
    "            vocab_dict[char] = char_bytes\n",
    "            char_tokens.append(byte)\n",
    "\n",
    "        else: \n",
    "            vocab_dict[char] = char_bytes\n",
    "\n",
    "            for b in char_bytes:\n",
    "                char_tokens.append(b)\n",
    "\n",
    "    return char_tokens\n",
    "\n",
    "\n",
    "byte_tokens = byte_char_encoding(string=string)\n",
    "print(\"byte tokens: \", byte_tokens)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "id": "1432cce6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "byter compression -> ,  1.0\n"
     ]
    }
   ],
   "source": [
    "print(\"byter compression -> , \", compression_ratio(string=string, \n",
    "                                                   tokens=byte_tokens))\n",
    "\n",
    "# is 1, which is the worst possible compression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "id": "da84acb6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'H'"
      ]
     },
     "execution_count": 116,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chr(72)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "id": "45a96050",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Word based tokenization\n",
    "# 1st define regez to split the sentence\n",
    "# Then split\n",
    "WORD_REGEX = r'([, !?])'  # Lets use just space for now\n",
    "\n",
    "import re\n",
    "sentence_word = re.split(pattern=WORD_REGEX, string=string)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "id": "33c4d615",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hello, üåç! ‰Ω†Â•Ω! \n",
      " ['Hello', ',', '', ' ', 'üåç', '!', '', ' ', '‰Ω†Â•Ω', '!', '']\n"
     ]
    }
   ],
   "source": [
    "print(string,\"\\n\", sentence_word) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "id": "cb838ac2",
   "metadata": {},
   "outputs": [],
   "source": [
    "clean_sentence = [word for word in sentence_word if word != \"\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "id": "0595a68c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Hello', ',', ' ', 'üåç', '!', ' ', '‰Ω†Â•Ω', '!']"
      ]
     },
     "execution_count": 141,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clean_sentence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15f4f127",
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab = {}\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "cs336-llms-from-scratch",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

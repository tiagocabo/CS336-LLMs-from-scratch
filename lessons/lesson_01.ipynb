{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 99,
   "id": "8e4e3308",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import GPT2Tokenizer\n",
    "\n",
    "tokenizer = GPT2Tokenizer.from_pretrained(\"gpt2\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "id": "21ae470e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'!': 0,\n",
       " '\"': 1,\n",
       " '#': 2,\n",
       " '$': 3,\n",
       " '%': 4,\n",
       " '&': 5,\n",
       " \"'\": 6,\n",
       " '(': 7,\n",
       " ')': 8,\n",
       " '*': 9,\n",
       " '+': 10,\n",
       " ',': 11,\n",
       " '-': 12,\n",
       " '.': 13,\n",
       " '/': 14,\n",
       " '0': 15,\n",
       " '1': 16,\n",
       " '2': 17,\n",
       " '3': 18,\n",
       " '4': 19,\n",
       " '5': 20,\n",
       " '6': 21,\n",
       " '7': 22,\n",
       " '8': 23,\n",
       " '9': 24,\n",
       " ':': 25,\n",
       " ';': 26,\n",
       " '<': 27,\n",
       " '=': 28,\n",
       " '>': 29,\n",
       " '?': 30,\n",
       " '@': 31,\n",
       " 'A': 32,\n",
       " 'B': 33,\n",
       " 'C': 34,\n",
       " 'D': 35,\n",
       " 'E': 36,\n",
       " 'F': 37,\n",
       " 'G': 38,\n",
       " 'H': 39,\n",
       " 'I': 40,\n",
       " 'J': 41,\n",
       " 'K': 42,\n",
       " 'L': 43,\n",
       " 'M': 44,\n",
       " 'N': 45,\n",
       " 'O': 46,\n",
       " 'P': 47,\n",
       " 'Q': 48,\n",
       " 'R': 49,\n",
       " 'S': 50,\n",
       " 'T': 51,\n",
       " 'U': 52,\n",
       " 'V': 53,\n",
       " 'W': 54,\n",
       " 'X': 55,\n",
       " 'Y': 56,\n",
       " 'Z': 57,\n",
       " '[': 58,\n",
       " '\\\\': 59,\n",
       " ']': 60,\n",
       " '^': 61,\n",
       " '_': 62,\n",
       " '`': 63,\n",
       " 'a': 64,\n",
       " 'b': 65,\n",
       " 'c': 66,\n",
       " 'd': 67,\n",
       " 'e': 68,\n",
       " 'f': 69,\n",
       " 'g': 70,\n",
       " 'h': 71,\n",
       " 'i': 72,\n",
       " 'j': 73,\n",
       " 'k': 74,\n",
       " 'l': 75,\n",
       " 'm': 76,\n",
       " 'n': 77,\n",
       " 'o': 78,\n",
       " 'p': 79,\n",
       " 'q': 80,\n",
       " 'r': 81,\n",
       " 's': 82,\n",
       " 't': 83,\n",
       " 'u': 84,\n",
       " 'v': 85,\n",
       " 'w': 86,\n",
       " 'x': 87,\n",
       " 'y': 88,\n",
       " 'z': 89,\n",
       " '{': 90,\n",
       " '|': 91,\n",
       " '}': 92,\n",
       " '~': 93,\n",
       " '¬°': 94,\n",
       " '¬¢': 95,\n",
       " '¬£': 96,\n",
       " '¬§': 97,\n",
       " '¬•': 98,\n",
       " '¬¶': 99,\n",
       " '¬ß': 100,\n",
       " '¬®': 101,\n",
       " '¬©': 102,\n",
       " '¬™': 103,\n",
       " '¬´': 104,\n",
       " '¬¨': 105,\n",
       " '¬Æ': 106,\n",
       " '¬Ø': 107,\n",
       " '¬∞': 108,\n",
       " '¬±': 109,\n",
       " '¬≤': 110,\n",
       " '¬≥': 111,\n",
       " '¬¥': 112,\n",
       " '¬µ': 113,\n",
       " '¬∂': 114,\n",
       " '¬∑': 115,\n",
       " '¬∏': 116,\n",
       " '¬π': 117,\n",
       " '¬∫': 118,\n",
       " '¬ª': 119,\n",
       " '¬º': 120,\n",
       " '¬Ω': 121,\n",
       " '¬æ': 122,\n",
       " '¬ø': 123,\n",
       " '√Ä': 124,\n",
       " '√Å': 125,\n",
       " '√Ç': 126,\n",
       " '√É': 127,\n",
       " '√Ñ': 128,\n",
       " '√Ö': 129,\n",
       " '√Ü': 130,\n",
       " '√á': 131,\n",
       " '√à': 132,\n",
       " '√â': 133,\n",
       " '√ä': 134,\n",
       " '√ã': 135,\n",
       " '√å': 136,\n",
       " '√ç': 137,\n",
       " '√é': 138,\n",
       " '√è': 139,\n",
       " '√ê': 140,\n",
       " '√ë': 141,\n",
       " '√í': 142,\n",
       " '√ì': 143,\n",
       " '√î': 144,\n",
       " '√ï': 145,\n",
       " '√ñ': 146,\n",
       " '√ó': 147,\n",
       " '√ò': 148,\n",
       " '√ô': 149,\n",
       " '√ö': 150,\n",
       " '√õ': 151,\n",
       " '√ú': 152,\n",
       " '√ù': 153,\n",
       " '√û': 154,\n",
       " '√ü': 155,\n",
       " '√†': 156,\n",
       " '√°': 157,\n",
       " '√¢': 158,\n",
       " '√£': 159,\n",
       " '√§': 160,\n",
       " '√•': 161,\n",
       " '√¶': 162,\n",
       " '√ß': 163,\n",
       " '√®': 164,\n",
       " '√©': 165,\n",
       " '√™': 166,\n",
       " '√´': 167,\n",
       " '√¨': 168,\n",
       " '√≠': 169,\n",
       " '√Æ': 170,\n",
       " '√Ø': 171,\n",
       " '√∞': 172,\n",
       " '√±': 173,\n",
       " '√≤': 174,\n",
       " '√≥': 175,\n",
       " '√¥': 176,\n",
       " '√µ': 177,\n",
       " '√∂': 178,\n",
       " '√∑': 179,\n",
       " '√∏': 180,\n",
       " '√π': 181,\n",
       " '√∫': 182,\n",
       " '√ª': 183,\n",
       " '√º': 184,\n",
       " '√Ω': 185,\n",
       " '√æ': 186,\n",
       " '√ø': 187,\n",
       " 'ƒÄ': 188,\n",
       " 'ƒÅ': 189,\n",
       " 'ƒÇ': 190,\n",
       " 'ƒÉ': 191,\n",
       " 'ƒÑ': 192,\n",
       " 'ƒÖ': 193,\n",
       " 'ƒÜ': 194,\n",
       " 'ƒá': 195,\n",
       " 'ƒà': 196,\n",
       " 'ƒâ': 197,\n",
       " 'ƒä': 198,\n",
       " 'ƒã': 199,\n",
       " 'ƒå': 200,\n",
       " 'ƒç': 201,\n",
       " 'ƒé': 202,\n",
       " 'ƒè': 203,\n",
       " 'ƒê': 204,\n",
       " 'ƒë': 205,\n",
       " 'ƒí': 206,\n",
       " 'ƒì': 207,\n",
       " 'ƒî': 208,\n",
       " 'ƒï': 209,\n",
       " 'ƒñ': 210,\n",
       " 'ƒó': 211,\n",
       " 'ƒò': 212,\n",
       " 'ƒô': 213,\n",
       " 'ƒö': 214,\n",
       " 'ƒõ': 215,\n",
       " 'ƒú': 216,\n",
       " 'ƒù': 217,\n",
       " 'ƒû': 218,\n",
       " 'ƒü': 219,\n",
       " 'ƒ†': 220,\n",
       " 'ƒ°': 221,\n",
       " 'ƒ¢': 222,\n",
       " 'ƒ£': 223,\n",
       " 'ƒ§': 224,\n",
       " 'ƒ•': 225,\n",
       " 'ƒ¶': 226,\n",
       " 'ƒß': 227,\n",
       " 'ƒ®': 228,\n",
       " 'ƒ©': 229,\n",
       " 'ƒ™': 230,\n",
       " 'ƒ´': 231,\n",
       " 'ƒ¨': 232,\n",
       " 'ƒ≠': 233,\n",
       " 'ƒÆ': 234,\n",
       " 'ƒØ': 235,\n",
       " 'ƒ∞': 236,\n",
       " 'ƒ±': 237,\n",
       " 'ƒ≤': 238,\n",
       " 'ƒ≥': 239,\n",
       " 'ƒ¥': 240,\n",
       " 'ƒµ': 241,\n",
       " 'ƒ∂': 242,\n",
       " 'ƒ∑': 243,\n",
       " 'ƒ∏': 244,\n",
       " 'ƒπ': 245,\n",
       " 'ƒ∫': 246,\n",
       " 'ƒª': 247,\n",
       " 'ƒº': 248,\n",
       " 'ƒΩ': 249,\n",
       " 'ƒæ': 250,\n",
       " 'ƒø': 251,\n",
       " '≈Ä': 252,\n",
       " '≈Å': 253,\n",
       " '≈Ç': 254,\n",
       " '≈É': 255,\n",
       " 'ƒ†t': 256,\n",
       " 'ƒ†a': 257,\n",
       " 'he': 258,\n",
       " 'in': 259,\n",
       " 're': 260,\n",
       " 'on': 261,\n",
       " 'ƒ†the': 262,\n",
       " 'er': 263,\n",
       " 'ƒ†s': 264,\n",
       " 'at': 265,\n",
       " 'ƒ†w': 266,\n",
       " 'ƒ†o': 267,\n",
       " 'en': 268,\n",
       " 'ƒ†c': 269,\n",
       " 'it': 270,\n",
       " 'is': 271,\n",
       " 'an': 272,\n",
       " 'or': 273,\n",
       " 'es': 274,\n",
       " 'ƒ†b': 275,\n",
       " 'ed': 276,\n",
       " 'ƒ†f': 277,\n",
       " 'ing': 278,\n",
       " 'ƒ†p': 279,\n",
       " 'ou': 280,\n",
       " 'ƒ†an': 281,\n",
       " 'al': 282,\n",
       " 'ar': 283,\n",
       " 'ƒ†to': 284,\n",
       " 'ƒ†m': 285,\n",
       " 'ƒ†of': 286,\n",
       " 'ƒ†in': 287,\n",
       " 'ƒ†d': 288,\n",
       " 'ƒ†h': 289,\n",
       " 'ƒ†and': 290,\n",
       " 'ic': 291,\n",
       " 'as': 292,\n",
       " 'le': 293,\n",
       " 'ƒ†th': 294,\n",
       " 'ion': 295,\n",
       " 'om': 296,\n",
       " 'll': 297,\n",
       " 'ent': 298,\n",
       " 'ƒ†n': 299,\n",
       " 'ƒ†l': 300,\n",
       " 'st': 301,\n",
       " 'ƒ†re': 302,\n",
       " 've': 303,\n",
       " 'ƒ†e': 304,\n",
       " 'ro': 305,\n",
       " 'ly': 306,\n",
       " 'ƒ†be': 307,\n",
       " 'ƒ†g': 308,\n",
       " 'ƒ†T': 309,\n",
       " 'ct': 310,\n",
       " 'ƒ†S': 311,\n",
       " 'id': 312,\n",
       " 'ot': 313,\n",
       " 'ƒ†I': 314,\n",
       " 'ut': 315,\n",
       " 'et': 316,\n",
       " 'ƒ†A': 317,\n",
       " 'ƒ†is': 318,\n",
       " 'ƒ†on': 319,\n",
       " 'im': 320,\n",
       " 'am': 321,\n",
       " 'ow': 322,\n",
       " 'ay': 323,\n",
       " 'ad': 324,\n",
       " 'se': 325,\n",
       " 'ƒ†that': 326,\n",
       " 'ƒ†C': 327,\n",
       " 'ig': 328,\n",
       " 'ƒ†for': 329,\n",
       " 'ac': 330,\n",
       " 'ƒ†y': 331,\n",
       " 'ver': 332,\n",
       " 'ur': 333,\n",
       " 'ƒ†u': 334,\n",
       " 'ld': 335,\n",
       " 'ƒ†st': 336,\n",
       " 'ƒ†M': 337,\n",
       " \"'s\": 338,\n",
       " 'ƒ†he': 339,\n",
       " 'ƒ†it': 340,\n",
       " 'ation': 341,\n",
       " 'ith': 342,\n",
       " 'ir': 343,\n",
       " 'ce': 344,\n",
       " 'ƒ†you': 345,\n",
       " 'il': 346,\n",
       " 'ƒ†B': 347,\n",
       " 'ƒ†wh': 348,\n",
       " 'ol': 349,\n",
       " 'ƒ†P': 350,\n",
       " 'ƒ†with': 351,\n",
       " 'ƒ†1': 352,\n",
       " 'ter': 353,\n",
       " 'ch': 354,\n",
       " 'ƒ†as': 355,\n",
       " 'ƒ†we': 356,\n",
       " 'ƒ†(': 357,\n",
       " 'nd': 358,\n",
       " 'ill': 359,\n",
       " 'ƒ†D': 360,\n",
       " 'if': 361,\n",
       " 'ƒ†2': 362,\n",
       " 'ag': 363,\n",
       " 'ers': 364,\n",
       " 'ke': 365,\n",
       " 'ƒ†\"': 366,\n",
       " 'ƒ†H': 367,\n",
       " 'em': 368,\n",
       " 'ƒ†con': 369,\n",
       " 'ƒ†W': 370,\n",
       " 'ƒ†R': 371,\n",
       " 'her': 372,\n",
       " 'ƒ†was': 373,\n",
       " 'ƒ†r': 374,\n",
       " 'od': 375,\n",
       " 'ƒ†F': 376,\n",
       " 'ul': 377,\n",
       " 'ate': 378,\n",
       " 'ƒ†at': 379,\n",
       " 'ri': 380,\n",
       " 'pp': 381,\n",
       " 'ore': 382,\n",
       " 'ƒ†The': 383,\n",
       " 'ƒ†se': 384,\n",
       " 'us': 385,\n",
       " 'ƒ†pro': 386,\n",
       " 'ƒ†ha': 387,\n",
       " 'um': 388,\n",
       " 'ƒ†are': 389,\n",
       " 'ƒ†de': 390,\n",
       " 'ain': 391,\n",
       " 'and': 392,\n",
       " 'ƒ†or': 393,\n",
       " 'igh': 394,\n",
       " 'est': 395,\n",
       " 'ist': 396,\n",
       " 'ab': 397,\n",
       " 'rom': 398,\n",
       " 'ƒ†N': 399,\n",
       " 'th': 400,\n",
       " 'ƒ†com': 401,\n",
       " 'ƒ†G': 402,\n",
       " 'un': 403,\n",
       " 'op': 404,\n",
       " '00': 405,\n",
       " 'ƒ†L': 406,\n",
       " 'ƒ†not': 407,\n",
       " 'ess': 408,\n",
       " 'ƒ†ex': 409,\n",
       " 'ƒ†v': 410,\n",
       " 'res': 411,\n",
       " 'ƒ†E': 412,\n",
       " 'ew': 413,\n",
       " 'ity': 414,\n",
       " 'ant': 415,\n",
       " 'ƒ†by': 416,\n",
       " 'el': 417,\n",
       " 'os': 418,\n",
       " 'ort': 419,\n",
       " 'oc': 420,\n",
       " 'qu': 421,\n",
       " 'ƒ†from': 422,\n",
       " 'ƒ†have': 423,\n",
       " 'ƒ†su': 424,\n",
       " 'ive': 425,\n",
       " 'ould': 426,\n",
       " 'ƒ†sh': 427,\n",
       " 'ƒ†this': 428,\n",
       " 'nt': 429,\n",
       " 'ra': 430,\n",
       " 'pe': 431,\n",
       " 'ight': 432,\n",
       " 'art': 433,\n",
       " 'ment': 434,\n",
       " 'ƒ†al': 435,\n",
       " 'ust': 436,\n",
       " 'end': 437,\n",
       " '--': 438,\n",
       " 'all': 439,\n",
       " 'ƒ†O': 440,\n",
       " 'ack': 441,\n",
       " 'ƒ†ch': 442,\n",
       " 'ƒ†le': 443,\n",
       " 'ies': 444,\n",
       " 'red': 445,\n",
       " 'ard': 446,\n",
       " '√¢ƒ¢': 447,\n",
       " 'out': 448,\n",
       " 'ƒ†J': 449,\n",
       " 'ƒ†ab': 450,\n",
       " 'ear': 451,\n",
       " 'iv': 452,\n",
       " 'ally': 453,\n",
       " 'our': 454,\n",
       " 'ost': 455,\n",
       " 'gh': 456,\n",
       " 'pt': 457,\n",
       " 'ƒ†pl': 458,\n",
       " 'ast': 459,\n",
       " 'ƒ†can': 460,\n",
       " 'ak': 461,\n",
       " 'ome': 462,\n",
       " 'ud': 463,\n",
       " 'The': 464,\n",
       " 'ƒ†his': 465,\n",
       " 'ƒ†do': 466,\n",
       " 'ƒ†go': 467,\n",
       " 'ƒ†has': 468,\n",
       " 'ge': 469,\n",
       " \"'t\": 470,\n",
       " 'ƒ†U': 471,\n",
       " 'rou': 472,\n",
       " 'ƒ†sa': 473,\n",
       " 'ƒ†j': 474,\n",
       " 'ƒ†but': 475,\n",
       " 'ƒ†wor': 476,\n",
       " 'ƒ†all': 477,\n",
       " 'ect': 478,\n",
       " 'ƒ†k': 479,\n",
       " 'ame': 480,\n",
       " 'ƒ†will': 481,\n",
       " 'ok': 482,\n",
       " 'ƒ†whe': 483,\n",
       " 'ƒ†they': 484,\n",
       " 'ide': 485,\n",
       " '01': 486,\n",
       " 'ff': 487,\n",
       " 'ich': 488,\n",
       " 'pl': 489,\n",
       " 'ther': 490,\n",
       " 'ƒ†tr': 491,\n",
       " '..': 492,\n",
       " 'ƒ†int': 493,\n",
       " 'ie': 494,\n",
       " 'ure': 495,\n",
       " 'age': 496,\n",
       " 'ƒ†ne': 497,\n",
       " 'ial': 498,\n",
       " 'ap': 499,\n",
       " 'ine': 500,\n",
       " 'ice': 501,\n",
       " 'ƒ†me': 502,\n",
       " 'ƒ†out': 503,\n",
       " 'ans': 504,\n",
       " 'one': 505,\n",
       " 'ong': 506,\n",
       " 'ions': 507,\n",
       " 'ƒ†who': 508,\n",
       " 'ƒ†K': 509,\n",
       " 'ƒ†up': 510,\n",
       " 'ƒ†their': 511,\n",
       " 'ƒ†ad': 512,\n",
       " 'ƒ†3': 513,\n",
       " 'ƒ†us': 514,\n",
       " 'ated': 515,\n",
       " 'ous': 516,\n",
       " 'ƒ†more': 517,\n",
       " 'ue': 518,\n",
       " 'og': 519,\n",
       " 'ƒ†St': 520,\n",
       " 'ind': 521,\n",
       " 'ike': 522,\n",
       " 'ƒ†so': 523,\n",
       " 'ime': 524,\n",
       " 'per': 525,\n",
       " '.\"': 526,\n",
       " 'ber': 527,\n",
       " 'iz': 528,\n",
       " 'act': 529,\n",
       " 'ƒ†one': 530,\n",
       " 'ƒ†said': 531,\n",
       " 'ƒ†-': 532,\n",
       " 'are': 533,\n",
       " 'ƒ†your': 534,\n",
       " 'cc': 535,\n",
       " 'ƒ†Th': 536,\n",
       " 'ƒ†cl': 537,\n",
       " 'ep': 538,\n",
       " 'ake': 539,\n",
       " 'able': 540,\n",
       " 'ip': 541,\n",
       " 'ƒ†cont': 542,\n",
       " 'ƒ†which': 543,\n",
       " 'ia': 544,\n",
       " 'ƒ†im': 545,\n",
       " 'ƒ†about': 546,\n",
       " 'ƒ†were': 547,\n",
       " 'very': 548,\n",
       " 'ub': 549,\n",
       " 'ƒ†had': 550,\n",
       " 'ƒ†en': 551,\n",
       " 'ƒ†comp': 552,\n",
       " ',\"': 553,\n",
       " 'ƒ†In': 554,\n",
       " 'ƒ†un': 555,\n",
       " 'ƒ†ag': 556,\n",
       " 'ire': 557,\n",
       " 'ace': 558,\n",
       " 'au': 559,\n",
       " 'ary': 560,\n",
       " 'ƒ†would': 561,\n",
       " 'ass': 562,\n",
       " 'ry': 563,\n",
       " 'ƒ†√¢ƒ¢': 564,\n",
       " 'cl': 565,\n",
       " 'ook': 566,\n",
       " 'ere': 567,\n",
       " 'so': 568,\n",
       " 'ƒ†V': 569,\n",
       " 'ign': 570,\n",
       " 'ib': 571,\n",
       " 'ƒ†off': 572,\n",
       " 'ƒ†te': 573,\n",
       " 'ven': 574,\n",
       " 'ƒ†Y': 575,\n",
       " 'ile': 576,\n",
       " 'ose': 577,\n",
       " 'ite': 578,\n",
       " 'orm': 579,\n",
       " 'ƒ†201': 580,\n",
       " 'ƒ†res': 581,\n",
       " 'ƒ†man': 582,\n",
       " 'ƒ†per': 583,\n",
       " 'ƒ†other': 584,\n",
       " 'ord': 585,\n",
       " 'ult': 586,\n",
       " 'ƒ†been': 587,\n",
       " 'ƒ†like': 588,\n",
       " 'ase': 589,\n",
       " 'ance': 590,\n",
       " 'ks': 591,\n",
       " 'ays': 592,\n",
       " 'own': 593,\n",
       " 'ence': 594,\n",
       " 'ƒ†dis': 595,\n",
       " 'ction': 596,\n",
       " 'ƒ†any': 597,\n",
       " 'ƒ†app': 598,\n",
       " 'ƒ†sp': 599,\n",
       " 'int': 600,\n",
       " 'ress': 601,\n",
       " 'ations': 602,\n",
       " 'ail': 603,\n",
       " 'ƒ†4': 604,\n",
       " 'ical': 605,\n",
       " 'ƒ†them': 606,\n",
       " 'ƒ†her': 607,\n",
       " 'ount': 608,\n",
       " 'ƒ†Ch': 609,\n",
       " 'ƒ†ar': 610,\n",
       " 'ƒ†if': 611,\n",
       " 'ƒ†there': 612,\n",
       " 'ƒ†pe': 613,\n",
       " 'ƒ†year': 614,\n",
       " 'av': 615,\n",
       " 'ƒ†my': 616,\n",
       " 'ƒ†some': 617,\n",
       " 'ƒ†when': 618,\n",
       " 'ough': 619,\n",
       " 'ach': 620,\n",
       " 'ƒ†than': 621,\n",
       " 'ru': 622,\n",
       " 'ond': 623,\n",
       " 'ick': 624,\n",
       " 'ƒ†over': 625,\n",
       " 'vel': 626,\n",
       " 'ƒ†qu': 627,\n",
       " 'ƒäƒä': 628,\n",
       " 'ƒ†sc': 629,\n",
       " 'reat': 630,\n",
       " 'ree': 631,\n",
       " 'ƒ†It': 632,\n",
       " 'ound': 633,\n",
       " 'port': 634,\n",
       " 'ƒ†also': 635,\n",
       " 'ƒ†part': 636,\n",
       " 'fter': 637,\n",
       " 'ƒ†kn': 638,\n",
       " 'ƒ†bec': 639,\n",
       " 'ƒ†time': 640,\n",
       " 'ens': 641,\n",
       " 'ƒ†5': 642,\n",
       " 'ople': 643,\n",
       " 'ƒ†what': 644,\n",
       " 'ƒ†no': 645,\n",
       " 'du': 646,\n",
       " 'mer': 647,\n",
       " 'ang': 648,\n",
       " 'ƒ†new': 649,\n",
       " '----': 650,\n",
       " 'ƒ†get': 651,\n",
       " 'ory': 652,\n",
       " 'ition': 653,\n",
       " 'ings': 654,\n",
       " 'ƒ†just': 655,\n",
       " 'ƒ†into': 656,\n",
       " 'ƒ†0': 657,\n",
       " 'ents': 658,\n",
       " 'ove': 659,\n",
       " 'te': 660,\n",
       " 'ƒ†people': 661,\n",
       " 'ƒ†pre': 662,\n",
       " 'ƒ†its': 663,\n",
       " 'ƒ†rec': 664,\n",
       " 'ƒ†tw': 665,\n",
       " 'ian': 666,\n",
       " 'irst': 667,\n",
       " 'ark': 668,\n",
       " 'ors': 669,\n",
       " 'ƒ†work': 670,\n",
       " 'ade': 671,\n",
       " 'ob': 672,\n",
       " 'ƒ†she': 673,\n",
       " 'ƒ†our': 674,\n",
       " 'wn': 675,\n",
       " 'ink': 676,\n",
       " 'lic': 677,\n",
       " 'ƒ†19': 678,\n",
       " 'ƒ†He': 679,\n",
       " 'ish': 680,\n",
       " 'nder': 681,\n",
       " 'ause': 682,\n",
       " 'ƒ†him': 683,\n",
       " 'ons': 684,\n",
       " 'ƒ†[': 685,\n",
       " 'ƒ†ro': 686,\n",
       " 'form': 687,\n",
       " 'ild': 688,\n",
       " 'ates': 689,\n",
       " 'vers': 690,\n",
       " 'ƒ†only': 691,\n",
       " 'oll': 692,\n",
       " 'ƒ†spe': 693,\n",
       " 'ck': 694,\n",
       " 'ell': 695,\n",
       " 'amp': 696,\n",
       " 'ƒ†acc': 697,\n",
       " 'ƒ†bl': 698,\n",
       " 'ious': 699,\n",
       " 'urn': 700,\n",
       " 'ft': 701,\n",
       " 'ood': 702,\n",
       " 'ƒ†how': 703,\n",
       " 'hed': 704,\n",
       " \"ƒ†'\": 705,\n",
       " 'ƒ†after': 706,\n",
       " 'aw': 707,\n",
       " 'ƒ†att': 708,\n",
       " 'ov': 709,\n",
       " 'ne': 710,\n",
       " 'ƒ†play': 711,\n",
       " 'erv': 712,\n",
       " 'ict': 713,\n",
       " 'ƒ†could': 714,\n",
       " 'itt': 715,\n",
       " 'ƒ†am': 716,\n",
       " 'ƒ†first': 717,\n",
       " 'ƒ†6': 718,\n",
       " 'ƒ†act': 719,\n",
       " 'ƒ†$': 720,\n",
       " 'ec': 721,\n",
       " 'hing': 722,\n",
       " 'ual': 723,\n",
       " 'ull': 724,\n",
       " 'ƒ†comm': 725,\n",
       " 'oy': 726,\n",
       " 'old': 727,\n",
       " 'ces': 728,\n",
       " 'ater': 729,\n",
       " 'ƒ†fe': 730,\n",
       " 'ƒ†bet': 731,\n",
       " 'we': 732,\n",
       " 'iff': 733,\n",
       " 'ƒ†two': 734,\n",
       " 'ock': 735,\n",
       " 'ƒ†back': 736,\n",
       " ').': 737,\n",
       " 'ident': 738,\n",
       " 'ƒ†under': 739,\n",
       " 'rough': 740,\n",
       " 'sel': 741,\n",
       " 'xt': 742,\n",
       " 'ƒ†may': 743,\n",
       " 'round': 744,\n",
       " 'ƒ†po': 745,\n",
       " 'ph': 746,\n",
       " 'iss': 747,\n",
       " 'ƒ†des': 748,\n",
       " 'ƒ†most': 749,\n",
       " 'ƒ†did': 750,\n",
       " 'ƒ†add': 751,\n",
       " 'ject': 752,\n",
       " 'ƒ†inc': 753,\n",
       " 'fore': 754,\n",
       " 'ƒ†pol': 755,\n",
       " 'ont': 756,\n",
       " 'ƒ†again': 757,\n",
       " 'clud': 758,\n",
       " 'tern': 759,\n",
       " 'ƒ†know': 760,\n",
       " 'ƒ†need': 761,\n",
       " 'ƒ†cons': 762,\n",
       " 'ƒ†co': 763,\n",
       " 'ƒ†.': 764,\n",
       " 'ƒ†want': 765,\n",
       " 'ƒ†see': 766,\n",
       " 'ƒ†7': 767,\n",
       " 'ning': 768,\n",
       " 'iew': 769,\n",
       " 'ƒ†This': 770,\n",
       " 'ced': 771,\n",
       " 'ƒ†even': 772,\n",
       " 'ƒ†ind': 773,\n",
       " 'ty': 774,\n",
       " 'ƒ†We': 775,\n",
       " 'ath': 776,\n",
       " 'ƒ†these': 777,\n",
       " 'ƒ†pr': 778,\n",
       " 'ƒ†use': 779,\n",
       " 'ƒ†because': 780,\n",
       " 'ƒ†fl': 781,\n",
       " 'ng': 782,\n",
       " 'ƒ†now': 783,\n",
       " 'ƒ†√¢ƒ¢ƒµ': 784,\n",
       " 'com': 785,\n",
       " 'ise': 786,\n",
       " 'ƒ†make': 787,\n",
       " 'ƒ†then': 788,\n",
       " 'ower': 789,\n",
       " 'ƒ†every': 790,\n",
       " 'ƒ†Un': 791,\n",
       " 'ƒ†sec': 792,\n",
       " 'oss': 793,\n",
       " 'uch': 794,\n",
       " 'ƒ†em': 795,\n",
       " 'ƒ†=': 796,\n",
       " 'ƒ†Re': 797,\n",
       " 'ied': 798,\n",
       " 'rit': 799,\n",
       " 'ƒ†inv': 800,\n",
       " 'lect': 801,\n",
       " 'ƒ†supp': 802,\n",
       " 'ating': 803,\n",
       " 'ƒ†look': 804,\n",
       " 'man': 805,\n",
       " 'pect': 806,\n",
       " 'ƒ†8': 807,\n",
       " 'row': 808,\n",
       " 'ƒ†bu': 809,\n",
       " 'ƒ†where': 810,\n",
       " 'ific': 811,\n",
       " 'ƒ†years': 812,\n",
       " 'ily': 813,\n",
       " 'ƒ†diff': 814,\n",
       " 'ƒ†should': 815,\n",
       " 'ƒ†rem': 816,\n",
       " 'Th': 817,\n",
       " 'In': 818,\n",
       " 'ƒ†ev': 819,\n",
       " 'day': 820,\n",
       " \"'re\": 821,\n",
       " 'rib': 822,\n",
       " 'ƒ†rel': 823,\n",
       " 'ss': 824,\n",
       " 'ƒ†def': 825,\n",
       " 'ƒ†right': 826,\n",
       " 'ƒ†sy': 827,\n",
       " '),': 828,\n",
       " 'les': 829,\n",
       " '000': 830,\n",
       " 'hen': 831,\n",
       " 'ƒ†through': 832,\n",
       " 'ƒ†Tr': 833,\n",
       " '__': 834,\n",
       " 'ƒ†way': 835,\n",
       " 'ƒ†don': 836,\n",
       " 'ƒ†,': 837,\n",
       " 'ƒ†10': 838,\n",
       " 'ased': 839,\n",
       " 'ƒ†ass': 840,\n",
       " 'ublic': 841,\n",
       " 'ƒ†reg': 842,\n",
       " 'ƒ†And': 843,\n",
       " 'ix': 844,\n",
       " 'ƒ†very': 845,\n",
       " 'ƒ†includ': 846,\n",
       " 'other': 847,\n",
       " 'ƒ†imp': 848,\n",
       " 'oth': 849,\n",
       " 'ƒ†sub': 850,\n",
       " 'ƒ†√¢ƒ¢ƒ∂': 851,\n",
       " 'ƒ†being': 852,\n",
       " 'arg': 853,\n",
       " 'ƒ†Wh': 854,\n",
       " '==': 855,\n",
       " 'ible': 856,\n",
       " 'ƒ†does': 857,\n",
       " 'ange': 858,\n",
       " 'ram': 859,\n",
       " 'ƒ†9': 860,\n",
       " 'ert': 861,\n",
       " 'ps': 862,\n",
       " 'ited': 863,\n",
       " 'ational': 864,\n",
       " 'ƒ†br': 865,\n",
       " 'ƒ†down': 866,\n",
       " 'ƒ†many': 867,\n",
       " 'aking': 868,\n",
       " 'ƒ†call': 869,\n",
       " 'uring': 870,\n",
       " 'ities': 871,\n",
       " 'ƒ†ph': 872,\n",
       " 'ics': 873,\n",
       " 'als': 874,\n",
       " 'ƒ†dec': 875,\n",
       " 'ative': 876,\n",
       " 'ener': 877,\n",
       " 'ƒ†before': 878,\n",
       " 'ility': 879,\n",
       " 'ƒ†well': 880,\n",
       " 'ƒ†much': 881,\n",
       " 'erson': 882,\n",
       " 'ƒ†those': 883,\n",
       " 'ƒ†such': 884,\n",
       " 'ƒ†ke': 885,\n",
       " 'ƒ†end': 886,\n",
       " 'ƒ†But': 887,\n",
       " 'ason': 888,\n",
       " 'ting': 889,\n",
       " 'ƒ†long': 890,\n",
       " 'ef': 891,\n",
       " 'ƒ†think': 892,\n",
       " 'ys': 893,\n",
       " 'ƒ†bel': 894,\n",
       " 'ƒ†sm': 895,\n",
       " 'its': 896,\n",
       " 'ax': 897,\n",
       " 'ƒ†own': 898,\n",
       " 'ƒ†prov': 899,\n",
       " 'ƒ†set': 900,\n",
       " 'ife': 901,\n",
       " 'ments': 902,\n",
       " 'ble': 903,\n",
       " 'ward': 904,\n",
       " 'ƒ†show': 905,\n",
       " 'ƒ†pres': 906,\n",
       " 'ms': 907,\n",
       " 'omet': 908,\n",
       " 'ƒ†ob': 909,\n",
       " 'ƒ†say': 910,\n",
       " 'ƒ†Sh': 911,\n",
       " 'ts': 912,\n",
       " 'ful': 913,\n",
       " 'ƒ†eff': 914,\n",
       " 'ƒ†gu': 915,\n",
       " 'ƒ†inst': 916,\n",
       " 'und': 917,\n",
       " 'ren': 918,\n",
       " 'cess': 919,\n",
       " 'ƒ†ent': 920,\n",
       " 'ƒ†You': 921,\n",
       " 'ƒ†good': 922,\n",
       " 'ƒ†start': 923,\n",
       " 'ince': 924,\n",
       " 'ƒ†made': 925,\n",
       " 'tt': 926,\n",
       " 'stem': 927,\n",
       " 'olog': 928,\n",
       " 'up': 929,\n",
       " 'ƒ†|': 930,\n",
       " 'ump': 931,\n",
       " 'ƒ†hel': 932,\n",
       " 'vern': 933,\n",
       " 'ular': 934,\n",
       " 'ually': 935,\n",
       " 'ƒ†ac': 936,\n",
       " 'ƒ†mon': 937,\n",
       " 'ƒ†last': 938,\n",
       " 'ƒ†200': 939,\n",
       " '10': 940,\n",
       " 'ƒ†stud': 941,\n",
       " 'ures': 942,\n",
       " 'ƒ†Ar': 943,\n",
       " 'self': 944,\n",
       " 'ars': 945,\n",
       " 'meric': 946,\n",
       " 'ues': 947,\n",
       " 'cy': 948,\n",
       " 'ƒ†min': 949,\n",
       " 'ollow': 950,\n",
       " 'ƒ†col': 951,\n",
       " 'io': 952,\n",
       " 'ƒ†mod': 953,\n",
       " 'ƒ†count': 954,\n",
       " 'ƒ†Com': 955,\n",
       " 'hes': 956,\n",
       " 'ƒ†fin': 957,\n",
       " 'air': 958,\n",
       " 'ier': 959,\n",
       " '√¢ƒ¢ƒ∂': 960,\n",
       " 'read': 961,\n",
       " 'ank': 962,\n",
       " 'atch': 963,\n",
       " 'ever': 964,\n",
       " 'ƒ†str': 965,\n",
       " 'ƒ†point': 966,\n",
       " 'ork': 967,\n",
       " 'ƒ†New': 968,\n",
       " 'ƒ†sur': 969,\n",
       " 'ool': 970,\n",
       " 'alk': 971,\n",
       " 'ement': 972,\n",
       " 'ƒ†used': 973,\n",
       " 'ract': 974,\n",
       " 'ween': 975,\n",
       " 'ƒ†same': 976,\n",
       " 'oun': 977,\n",
       " 'ƒ†Al': 978,\n",
       " 'ci': 979,\n",
       " 'ƒ†differe': 980,\n",
       " 'ƒ†while': 981,\n",
       " '--------': 982,\n",
       " 'ƒ†game': 983,\n",
       " 'cept': 984,\n",
       " 'ƒ†sim': 985,\n",
       " '...': 986,\n",
       " 'ƒ†inter': 987,\n",
       " 'ek': 988,\n",
       " 'ƒ†report': 989,\n",
       " 'ƒ†produ': 990,\n",
       " 'ƒ†still': 991,\n",
       " 'led': 992,\n",
       " 'ah': 993,\n",
       " 'ƒ†here': 994,\n",
       " 'ƒ†world': 995,\n",
       " 'ƒ†though': 996,\n",
       " 'ƒ†num': 997,\n",
       " 'arch': 998,\n",
       " 'imes': 999,\n",
       " ...}"
      ]
     },
     "execution_count": 100,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.get_vocab()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "beef3eb8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[15496, 11, 12520, 234, 235, 0, 220, 19526, 254, 25001, 121, 0]\n"
     ]
    }
   ],
   "source": [
    "string = \"Hello, üåç! ‰Ω†Â•Ω!\"\n",
    "print(tokenizer.encode(string))\n",
    "tokens = tokenizer.encode(string)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "2ebdad81",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "15496 ->  Hello\n",
      "11 ->  ,\n",
      "12520 ->   ÔøΩ\n",
      "234 ->  ÔøΩ\n",
      "235 ->  ÔøΩ\n",
      "0 ->  !\n",
      "220 ->   \n",
      "19526 ->  ÔøΩ\n",
      "254 ->  ÔøΩ\n",
      "25001 ->  ÔøΩ\n",
      "121 ->  ÔøΩ\n",
      "0 ->  !\n"
     ]
    }
   ],
   "source": [
    "# decode single value\n",
    "for token in tokens:\n",
    "    print(f\"{token} -> \", tokenizer.decode(token))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "a8f78a7c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compression_ratio(string:str, tokens:list):\n",
    "    original_size = len(bytes(string, encoding=\"utf-8\"))\n",
    "    num_tokens = len(tokens)\n",
    "    return original_size/num_tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "c8b310fb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "original_size ->  20\n",
      "num_tokens ->  12\n",
      "compression ->  1.6666666666666667\n",
      "compression_formula ->  1.6666666666666667\n"
     ]
    }
   ],
   "source": [
    "# compression ratio\n",
    "original_size = len(bytes(string, encoding=\"utf-8\"))\n",
    "num_tokens = len(tokens)\n",
    "\n",
    "\n",
    "print(\"original_size -> \", original_size)\n",
    "print(\"num_tokens -> \", num_tokens)\n",
    "print(\"compression -> \", original_size/num_tokens)\n",
    "print(\"compression_formula -> \", compression_ratio(string=string, tokens=tokens))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "6b68db19",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "H\n",
      "e\n",
      "l\n",
      "l\n",
      "o\n",
      ",\n",
      " \n",
      "üåç\n",
      "!\n",
      " \n",
      "‰Ω†\n",
      "Â•Ω\n",
      "!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "({'H': 0,\n",
       "  'e': 1,\n",
       "  'l': 2,\n",
       "  'o': 3,\n",
       "  ',': 4,\n",
       "  ' ': 5,\n",
       "  'üåç': 6,\n",
       "  '!': 7,\n",
       "  '‰Ω†': 8,\n",
       "  'Â•Ω': 9},\n",
       " [0, 1, 2, 2, 3, 4, 5, 6, 7, 5, 8, 9, 7])"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Options for tokenizes\n",
    "# Option 1: Each character a token\n",
    "# Lets then map each character to a number\n",
    "# do I need catalog? \n",
    "\n",
    "vocab_dict = {}\n",
    "index_cnt = 0\n",
    "char_tokens = []\n",
    "for char in string: \n",
    "    print(char)\n",
    "    if char not in vocab_dict:\n",
    "        vocab_dict[char] = index_cnt\n",
    "        char_tokens.append(index_cnt)\n",
    "        index_cnt += 1\n",
    "\n",
    "    else:\n",
    "        char_tokens.append(vocab_dict[char])\n",
    "\n",
    "#tokens\n",
    "vocab_dict, char_tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "b980ddff",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "char compression -> ,  1.5384615384615385\n"
     ]
    }
   ],
   "source": [
    "print(\"char compression -> , \", compression_ratio(string=string, tokens=char_tokens))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "14470bb2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "97"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ord(\"a\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "af02d31b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'a'"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chr(97)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "5346ee6e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "b'a'\n",
      "b'\\xf0\\x9f\\x8c\\x8e'\n"
     ]
    }
   ],
   "source": [
    "# Byte-based tokenization\n",
    "# unicode strings can be represented as integers between 0 and 255\n",
    "# utf-8 representation\n",
    "print(bytes(\"a\", encoding=\"utf-8\"))\n",
    "print(bytes('üåé', encoding=\"utf-8\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "96505f40",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "240"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "int.from_bytes(b'\\xf0')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "ed30dcbf",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(bytes('üåé', encoding=\"utf-8\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "d86c432a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "byte tokens:  [72, 101, 108, 108, 111, 44, 32, 240, 159, 140, 141, 33, 32, 228, 189, 160, 229, 165, 189, 33]\n"
     ]
    }
   ],
   "source": [
    "def byte_char_encoding(string:str):\n",
    "    \"\"\" Splits string in separate characters\n",
    "     Split in single bytes, case a character is represented by more than 1 byte. \n",
    "       \"\"\"\n",
    "    vocab_dict = {}\n",
    "\n",
    "    char_tokens = []\n",
    "    for char in string: \n",
    "        #print(bytes(char, encoding=\"utf-8\"))\n",
    "        char_bytes = bytes(char, encoding=\"utf-8\")\n",
    "        if len(char_bytes) == 0:\n",
    "            byte = char_bytes[0]\n",
    "            vocab_dict[char] = char_bytes\n",
    "            char_tokens.append(byte)\n",
    "\n",
    "        else: \n",
    "            vocab_dict[char] = char_bytes\n",
    "\n",
    "            for b in char_bytes:\n",
    "                char_tokens.append(b)\n",
    "\n",
    "    return char_tokens\n",
    "\n",
    "\n",
    "byte_tokens = byte_char_encoding(string=string)\n",
    "print(\"byte tokens: \", byte_tokens)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "1432cce6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "byter compression -> ,  1.0\n"
     ]
    }
   ],
   "source": [
    "print(\"byter compression -> , \", compression_ratio(string=string, \n",
    "                                                   tokens=byte_tokens))\n",
    "\n",
    "# is 1, which is the worst possible compression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "da84acb6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'H'"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chr(72)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "45a96050",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Word based tokenization\n",
    "# 1st define regez to split the sentence\n",
    "# Then split\n",
    "WORD_REGEX = r'([, !?])'  # Lets use just space for now\n",
    "\n",
    "import re\n",
    "sentence_word = re.split(pattern=WORD_REGEX, string=string)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "33c4d615",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hello, üåç! ‰Ω†Â•Ω! \n",
      " ['Hello', ',', '', ' ', 'üåç', '!', '', ' ', '‰Ω†Â•Ω', '!', '']\n"
     ]
    }
   ],
   "source": [
    "print(string,\"\\n\", sentence_word) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "cb838ac2",
   "metadata": {},
   "outputs": [],
   "source": [
    "clean_sentence = [word for word in sentence_word if word != \"\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "0595a68c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Hello', ',', ' ', 'üåç', '!', ' ', '‰Ω†Â•Ω', '!']"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clean_sentence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "15f4f127",
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab = {}\n",
    "index = 0\n",
    "word_tokens = []\n",
    "for word in clean_sentence:\n",
    "    if word not in vocab: \n",
    "        vocab[word] = index\n",
    "        word_tokens.append(index)\n",
    "        index += 1\n",
    "    else:\n",
    "        word_tokens.append(vocab[word])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "b0b96a27",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(['Hello', ',', ' ', 'üåç', '!', ' ', '‰Ω†Â•Ω', '!'], 'Hello, üåç! ‰Ω†Â•Ω!')"
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clean_sentence, string"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "5ee7cba5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0, 1, 2, 3, 4, 2, 5, 4]"
      ]
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "word_tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "c7750376",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2.5"
      ]
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "compression_ratio(string=string, tokens=word_tokens)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ed9debaa",
   "metadata": {},
   "source": [
    "# Byte Pair Encoding (BPE)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "0069b807",
   "metadata": {},
   "outputs": [],
   "source": [
    "#### Step 1: Convert each char into a single byte\n",
    "#### Step 2: Count each adjacent pair\n",
    "#### Step 3: Merge the top1 most frequent\n",
    "#### Step 4: Repeat with a fix limit of merges ??? \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "52169601",
   "metadata": {},
   "outputs": [],
   "source": [
    "sentence = \"the cat is in the hat\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "4771f5a3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "127758"
      ]
     },
     "execution_count": 68,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bytes('üåé', encoding=\"utf-8\")\n",
    "ord('üåé')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "02a6f25c",
   "metadata": {},
   "outputs": [],
   "source": [
    "char_bytes = [ord(char) for char in sentence]\n",
    "\n",
    "counts_dict = {}\n",
    "for i in range(len(char_bytes)-1):\n",
    "    pair =  (char_bytes[i],char_bytes[i+1])\n",
    "    if pair not in counts_dict:\n",
    "        counts_dict[pair] = 0\n",
    "    else: \n",
    "        counts_dict[pair] += 1\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "71d9c9d9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{(116, 104): 1, (104, 101): 1, (101, 32): 1, (97, 116): 1, (32, 105): 1, (32, 99): 0, (99, 97): 0, (116, 32): 0, (105, 115): 0, (115, 32): 0, (105, 110): 0, (110, 32): 0, (32, 116): 0, (32, 104): 0, (104, 97): 0}\n"
     ]
    }
   ],
   "source": [
    "counts_dict = {k: v for k, v in sorted(counts_dict.items(), key=lambda item: item[1], reverse=True)}\n",
    "print(counts_dict)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "428647ee",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{(116, 104): 1,\n",
       " (104, 101): 1,\n",
       " (101, 32): 1,\n",
       " (97, 116): 1,\n",
       " (32, 105): 1,\n",
       " (32, 99): 0,\n",
       " (99, 97): 0,\n",
       " (116, 32): 0,\n",
       " (105, 115): 0,\n",
       " (115, 32): 0,\n",
       " (105, 110): 0,\n",
       " (110, 32): 0,\n",
       " (32, 116): 0,\n",
       " (32, 104): 0,\n",
       " (104, 97): 0}"
      ]
     },
     "execution_count": 71,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "counts_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "9a894a1f",
   "metadata": {},
   "outputs": [],
   "source": [
    "to_merge = next(iter(counts_dict))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "05c22f82",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(116, 104)"
      ]
     },
     "execution_count": 73,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "to_merge"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "c87eec62",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "104"
      ]
     },
     "execution_count": 74,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# merge\n",
    "next_token = 257\n",
    "merge_position = char_bytes.index(to_merge[0])\n",
    "\n",
    "char_bytes.pop(merge_position)\n",
    "# repeat since the list will move left \n",
    "char_bytes.pop(merge_position)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "id": "79c6c583",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[101,\n",
       " 32,\n",
       " 99,\n",
       " 97,\n",
       " 116,\n",
       " 32,\n",
       " 105,\n",
       " 115,\n",
       " 32,\n",
       " 105,\n",
       " 110,\n",
       " 32,\n",
       " 116,\n",
       " 104,\n",
       " 101,\n",
       " 32,\n",
       " 104,\n",
       " 97,\n",
       " 116]"
      ]
     },
     "execution_count": 75,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "char_bytes "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "id": "91705dbe",
   "metadata": {},
   "outputs": [],
   "source": [
    "# insert new token on the removed indexes\n",
    "char_bytes.insert(merge_position, next_token)\n",
    "next_token += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "id": "c389fb70",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[257,\n",
       " 101,\n",
       " 32,\n",
       " 99,\n",
       " 97,\n",
       " 116,\n",
       " 32,\n",
       " 105,\n",
       " 115,\n",
       " 32,\n",
       " 105,\n",
       " 110,\n",
       " 32,\n",
       " 116,\n",
       " 104,\n",
       " 101,\n",
       " 32,\n",
       " 104,\n",
       " 97,\n",
       " 116]"
      ]
     },
     "execution_count": 77,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "char_bytes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4aca2042",
   "metadata": {},
   "outputs": [],
   "source": [
    "#### Step 1: Convert each char into a single byte\n",
    "\n",
    "class BytePairEncoder:\n",
    "    \n",
    "    def __init__(self, num_merges:int=10):\n",
    "        self.num_merges = num_merges\n",
    "        self.vocab = {}\n",
    "        self.token_dict = {}\n",
    "\n",
    "    def train(self):\n",
    "        pass\n",
    "\n",
    "    \n",
    "    def encode(self, sentence:str):\n",
    "        NEXT_TOKEN = 256\n",
    "        char_bytes = [ord(char) for char in sentence]\n",
    "        for _ in range(self.num_merges):\n",
    "            new_chars = []\n",
    "\n",
    "            # pairs counter\n",
    "            counts_dict = {}\n",
    "            for i in range(len(char_bytes)-1):\n",
    "                pair =  (char_bytes[i],char_bytes[i+1])\n",
    "                if pair not in counts_dict:\n",
    "                    counts_dict[pair] = 1\n",
    "                else: \n",
    "                    counts_dict[pair] += 1\n",
    "            \n",
    "\n",
    "            # order\n",
    "            counts_dict = {k: v for k, v in sorted(counts_dict.items(), key=lambda item: item[1], reverse=True)}\n",
    "\n",
    "            # find pair to merge\n",
    "            to_merge_key = next(iter(counts_dict))\n",
    "            print(\"to merge:  \", to_merge_key)\n",
    "\n",
    "            \n",
    "            # remove merged\n",
    "            pointer = 0\n",
    "            while pointer < len(char_bytes)-1: \n",
    "                if char_bytes[pointer] ==  to_merge_key[0] and char_bytes[pointer + 1] == to_merge_key[1]:\n",
    "                    new_chars.append(NEXT_TOKEN)\n",
    "                    pointer += 2\n",
    "\n",
    "                else: \n",
    "                    new_chars.append(char_bytes[pointer])\n",
    "                    pointer += 1\n",
    "            NEXT_TOKEN += 1\n",
    "            if pointer != len(char_bytes):\n",
    "                new_chars.append(char_bytes[pointer])\n",
    "            \n",
    "            char_bytes = new_chars\n",
    "            \n",
    "        return char_bytes\n",
    "\n",
    "\n",
    "    def decode(self):\n",
    "        pass\n",
    "    \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "id": "0fc195cf",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "20"
      ]
     },
     "execution_count": 79,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(char_bytes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "id": "3e208ea1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'the cat is in the hat'"
      ]
     },
     "execution_count": 80,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sentence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "id": "6c9dc383",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[256, 101, 32, 99, 97, 116, 32, 105, 115, 32, 105, 110, 32, 256, 101, 32, 104, 97, 116]\n"
     ]
    }
   ],
   "source": [
    "from bpetokenizer import BPETokenizer\n",
    "tokenizer = BPETokenizer()\n",
    "tokenizer.train(sentence, vocab_size=257)\n",
    "print(tokenizer.encode(sentence))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "id": "dd966116",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "to merge:   (116, 104)\n",
      "[256, 101, 32, 99, 97, 116, 32, 105, 115, 32, 105, 110, 32, 256, 101, 32, 104, 97, 116] 19\n",
      "to merge:   (116, 104)\n",
      "to merge:   (256, 101)\n",
      "[257, 32, 99, 97, 116, 32, 105, 115, 32, 105, 110, 32, 257, 32, 104, 97, 116] 17\n",
      "to merge:   (116, 104)\n",
      "to merge:   (256, 101)\n",
      "to merge:   (257, 32)\n",
      "[258, 99, 97, 116, 32, 105, 115, 32, 105, 110, 32, 258, 104, 97, 116] 15\n"
     ]
    }
   ],
   "source": [
    "bpe = BytePairEncoder(num_merges=1)\n",
    "result = bpe.encode(sentence=sentence)\n",
    "print(result, len(result))\n",
    "bpe = BytePairEncoder(num_merges=2)\n",
    "result = bpe.encode(sentence=sentence)\n",
    "print(result, len(result))\n",
    "\n",
    "bpe = BytePairEncoder(num_merges=3)\n",
    "result = bpe.encode(sentence=sentence)\n",
    "print(result, len(result))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "id": "08eebecf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "to merge:   (116, 104)\n",
      "[256, 101, 32, 99, 97, 116, 32, 105, 115, 32, 105, 110, 32, 256, 101, 32, 104, 97, 116]\n",
      "[256, 101, 32, 99, 97, 116, 32, 105, 115, 32, 105, 110, 32, 256, 101, 32, 104, 97, 116]\n"
     ]
    }
   ],
   "source": [
    "print(bpe.encode(sentence=sentence))\n",
    "print(tokenizer.encode(sentence))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2feee059",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "vocab is  257\n",
      "to merge:   (116, 104)\n",
      "my bpe  [256, 101, 32, 99, 97, 116, 32, 105, 115, 32, 105, 110, 32, 256, 101, 32, 104, 97, 116]\n",
      "   lib: [256, 101, 32, 99, 97, 116, 32, 105, 115, 32, 105, 110, 32, 256, 101, 32, 104, 97, 116]\n",
      "vocab is  258\n",
      "to merge:   (116, 104)\n",
      "to merge:   (256, 101)\n",
      "my bpe  [257, 32, 99, 97, 116, 32, 105, 115, 32, 105, 110, 32, 257, 32, 104, 97, 116]\n",
      "   lib: [257, 32, 99, 97, 116, 32, 105, 115, 32, 105, 110, 32, 257, 32, 104, 97, 116]\n",
      "vocab is  259\n",
      "to merge:   (116, 104)\n",
      "to merge:   (256, 101)\n",
      "to merge:   (257, 32)\n",
      "my bpe  [258, 99, 97, 116, 32, 105, 115, 32, 105, 110, 32, 258, 104, 97, 116]\n",
      "   lib: [257, 32, 99, 258, 32, 105, 115, 32, 105, 110, 32, 257, 32, 104, 258]\n",
      "vocab is  260\n",
      "to merge:   (116, 104)\n",
      "to merge:   (256, 101)\n",
      "to merge:   (257, 32)\n",
      "to merge:   (97, 116)\n",
      "my bpe  [258, 99, 259, 32, 105, 115, 32, 105, 110, 32, 258, 104, 259]\n",
      "   lib: [257, 32, 99, 258, 259, 115, 259, 110, 32, 257, 32, 104, 258]\n",
      "vocab is  261\n",
      "to merge:   (116, 104)\n",
      "to merge:   (256, 101)\n",
      "to merge:   (257, 32)\n",
      "to merge:   (97, 116)\n",
      "to merge:   (32, 105)\n",
      "my bpe  [258, 99, 259, 260, 115, 260, 110, 32, 258, 104, 259]\n",
      "   lib: [257, 260, 258, 259, 115, 259, 110, 32, 257, 32, 104, 258]\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "num_merges = 5\n",
    "for i in range(1,num_merges+1):\n",
    "    vocab = 256 + i\n",
    "    print(\"vocab is \", vocab)\n",
    "    tokenizer = BPETokenizer()\n",
    "    tic = time.time()\n",
    "    tokenizer.train(sentence, vocab_size=vocab)\n",
    "    toc = time.time()\n",
    "    print(\"my bpe takes: \", toc-tic)\n",
    "    bpe = BytePairEncoder(num_merges=i)\n",
    "    print(\"my bpe \", bpe.encode(sentence=sentence))\n",
    "    print(\"   lib:\", tokenizer.encode(sentence))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "847393b4",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "cs336-llms-from-scratch",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

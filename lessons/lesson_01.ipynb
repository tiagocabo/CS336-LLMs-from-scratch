{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 99,
   "id": "8e4e3308",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import GPT2Tokenizer\n",
    "\n",
    "tokenizer = GPT2Tokenizer.from_pretrained(\"gpt2\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "id": "21ae470e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'!': 0,\n",
       " '\"': 1,\n",
       " '#': 2,\n",
       " '$': 3,\n",
       " '%': 4,\n",
       " '&': 5,\n",
       " \"'\": 6,\n",
       " '(': 7,\n",
       " ')': 8,\n",
       " '*': 9,\n",
       " '+': 10,\n",
       " ',': 11,\n",
       " '-': 12,\n",
       " '.': 13,\n",
       " '/': 14,\n",
       " '0': 15,\n",
       " '1': 16,\n",
       " '2': 17,\n",
       " '3': 18,\n",
       " '4': 19,\n",
       " '5': 20,\n",
       " '6': 21,\n",
       " '7': 22,\n",
       " '8': 23,\n",
       " '9': 24,\n",
       " ':': 25,\n",
       " ';': 26,\n",
       " '<': 27,\n",
       " '=': 28,\n",
       " '>': 29,\n",
       " '?': 30,\n",
       " '@': 31,\n",
       " 'A': 32,\n",
       " 'B': 33,\n",
       " 'C': 34,\n",
       " 'D': 35,\n",
       " 'E': 36,\n",
       " 'F': 37,\n",
       " 'G': 38,\n",
       " 'H': 39,\n",
       " 'I': 40,\n",
       " 'J': 41,\n",
       " 'K': 42,\n",
       " 'L': 43,\n",
       " 'M': 44,\n",
       " 'N': 45,\n",
       " 'O': 46,\n",
       " 'P': 47,\n",
       " 'Q': 48,\n",
       " 'R': 49,\n",
       " 'S': 50,\n",
       " 'T': 51,\n",
       " 'U': 52,\n",
       " 'V': 53,\n",
       " 'W': 54,\n",
       " 'X': 55,\n",
       " 'Y': 56,\n",
       " 'Z': 57,\n",
       " '[': 58,\n",
       " '\\\\': 59,\n",
       " ']': 60,\n",
       " '^': 61,\n",
       " '_': 62,\n",
       " '`': 63,\n",
       " 'a': 64,\n",
       " 'b': 65,\n",
       " 'c': 66,\n",
       " 'd': 67,\n",
       " 'e': 68,\n",
       " 'f': 69,\n",
       " 'g': 70,\n",
       " 'h': 71,\n",
       " 'i': 72,\n",
       " 'j': 73,\n",
       " 'k': 74,\n",
       " 'l': 75,\n",
       " 'm': 76,\n",
       " 'n': 77,\n",
       " 'o': 78,\n",
       " 'p': 79,\n",
       " 'q': 80,\n",
       " 'r': 81,\n",
       " 's': 82,\n",
       " 't': 83,\n",
       " 'u': 84,\n",
       " 'v': 85,\n",
       " 'w': 86,\n",
       " 'x': 87,\n",
       " 'y': 88,\n",
       " 'z': 89,\n",
       " '{': 90,\n",
       " '|': 91,\n",
       " '}': 92,\n",
       " '~': 93,\n",
       " 'Â¡': 94,\n",
       " 'Â¢': 95,\n",
       " 'Â£': 96,\n",
       " 'Â¤': 97,\n",
       " 'Â¥': 98,\n",
       " 'Â¦': 99,\n",
       " 'Â§': 100,\n",
       " 'Â¨': 101,\n",
       " 'Â©': 102,\n",
       " 'Âª': 103,\n",
       " 'Â«': 104,\n",
       " 'Â¬': 105,\n",
       " 'Â®': 106,\n",
       " 'Â¯': 107,\n",
       " 'Â°': 108,\n",
       " 'Â±': 109,\n",
       " 'Â²': 110,\n",
       " 'Â³': 111,\n",
       " 'Â´': 112,\n",
       " 'Âµ': 113,\n",
       " 'Â¶': 114,\n",
       " 'Â·': 115,\n",
       " 'Â¸': 116,\n",
       " 'Â¹': 117,\n",
       " 'Âº': 118,\n",
       " 'Â»': 119,\n",
       " 'Â¼': 120,\n",
       " 'Â½': 121,\n",
       " 'Â¾': 122,\n",
       " 'Â¿': 123,\n",
       " 'Ã€': 124,\n",
       " 'Ã': 125,\n",
       " 'Ã‚': 126,\n",
       " 'Ãƒ': 127,\n",
       " 'Ã„': 128,\n",
       " 'Ã…': 129,\n",
       " 'Ã†': 130,\n",
       " 'Ã‡': 131,\n",
       " 'Ãˆ': 132,\n",
       " 'Ã‰': 133,\n",
       " 'ÃŠ': 134,\n",
       " 'Ã‹': 135,\n",
       " 'ÃŒ': 136,\n",
       " 'Ã': 137,\n",
       " 'ÃŽ': 138,\n",
       " 'Ã': 139,\n",
       " 'Ã': 140,\n",
       " 'Ã‘': 141,\n",
       " 'Ã’': 142,\n",
       " 'Ã“': 143,\n",
       " 'Ã”': 144,\n",
       " 'Ã•': 145,\n",
       " 'Ã–': 146,\n",
       " 'Ã—': 147,\n",
       " 'Ã˜': 148,\n",
       " 'Ã™': 149,\n",
       " 'Ãš': 150,\n",
       " 'Ã›': 151,\n",
       " 'Ãœ': 152,\n",
       " 'Ã': 153,\n",
       " 'Ãž': 154,\n",
       " 'ÃŸ': 155,\n",
       " 'Ã ': 156,\n",
       " 'Ã¡': 157,\n",
       " 'Ã¢': 158,\n",
       " 'Ã£': 159,\n",
       " 'Ã¤': 160,\n",
       " 'Ã¥': 161,\n",
       " 'Ã¦': 162,\n",
       " 'Ã§': 163,\n",
       " 'Ã¨': 164,\n",
       " 'Ã©': 165,\n",
       " 'Ãª': 166,\n",
       " 'Ã«': 167,\n",
       " 'Ã¬': 168,\n",
       " 'Ã­': 169,\n",
       " 'Ã®': 170,\n",
       " 'Ã¯': 171,\n",
       " 'Ã°': 172,\n",
       " 'Ã±': 173,\n",
       " 'Ã²': 174,\n",
       " 'Ã³': 175,\n",
       " 'Ã´': 176,\n",
       " 'Ãµ': 177,\n",
       " 'Ã¶': 178,\n",
       " 'Ã·': 179,\n",
       " 'Ã¸': 180,\n",
       " 'Ã¹': 181,\n",
       " 'Ãº': 182,\n",
       " 'Ã»': 183,\n",
       " 'Ã¼': 184,\n",
       " 'Ã½': 185,\n",
       " 'Ã¾': 186,\n",
       " 'Ã¿': 187,\n",
       " 'Ä€': 188,\n",
       " 'Ä': 189,\n",
       " 'Ä‚': 190,\n",
       " 'Äƒ': 191,\n",
       " 'Ä„': 192,\n",
       " 'Ä…': 193,\n",
       " 'Ä†': 194,\n",
       " 'Ä‡': 195,\n",
       " 'Äˆ': 196,\n",
       " 'Ä‰': 197,\n",
       " 'ÄŠ': 198,\n",
       " 'Ä‹': 199,\n",
       " 'ÄŒ': 200,\n",
       " 'Ä': 201,\n",
       " 'ÄŽ': 202,\n",
       " 'Ä': 203,\n",
       " 'Ä': 204,\n",
       " 'Ä‘': 205,\n",
       " 'Ä’': 206,\n",
       " 'Ä“': 207,\n",
       " 'Ä”': 208,\n",
       " 'Ä•': 209,\n",
       " 'Ä–': 210,\n",
       " 'Ä—': 211,\n",
       " 'Ä˜': 212,\n",
       " 'Ä™': 213,\n",
       " 'Äš': 214,\n",
       " 'Ä›': 215,\n",
       " 'Äœ': 216,\n",
       " 'Ä': 217,\n",
       " 'Äž': 218,\n",
       " 'ÄŸ': 219,\n",
       " 'Ä ': 220,\n",
       " 'Ä¡': 221,\n",
       " 'Ä¢': 222,\n",
       " 'Ä£': 223,\n",
       " 'Ä¤': 224,\n",
       " 'Ä¥': 225,\n",
       " 'Ä¦': 226,\n",
       " 'Ä§': 227,\n",
       " 'Ä¨': 228,\n",
       " 'Ä©': 229,\n",
       " 'Äª': 230,\n",
       " 'Ä«': 231,\n",
       " 'Ä¬': 232,\n",
       " 'Ä­': 233,\n",
       " 'Ä®': 234,\n",
       " 'Ä¯': 235,\n",
       " 'Ä°': 236,\n",
       " 'Ä±': 237,\n",
       " 'Ä²': 238,\n",
       " 'Ä³': 239,\n",
       " 'Ä´': 240,\n",
       " 'Äµ': 241,\n",
       " 'Ä¶': 242,\n",
       " 'Ä·': 243,\n",
       " 'Ä¸': 244,\n",
       " 'Ä¹': 245,\n",
       " 'Äº': 246,\n",
       " 'Ä»': 247,\n",
       " 'Ä¼': 248,\n",
       " 'Ä½': 249,\n",
       " 'Ä¾': 250,\n",
       " 'Ä¿': 251,\n",
       " 'Å€': 252,\n",
       " 'Å': 253,\n",
       " 'Å‚': 254,\n",
       " 'Åƒ': 255,\n",
       " 'Ä t': 256,\n",
       " 'Ä a': 257,\n",
       " 'he': 258,\n",
       " 'in': 259,\n",
       " 're': 260,\n",
       " 'on': 261,\n",
       " 'Ä the': 262,\n",
       " 'er': 263,\n",
       " 'Ä s': 264,\n",
       " 'at': 265,\n",
       " 'Ä w': 266,\n",
       " 'Ä o': 267,\n",
       " 'en': 268,\n",
       " 'Ä c': 269,\n",
       " 'it': 270,\n",
       " 'is': 271,\n",
       " 'an': 272,\n",
       " 'or': 273,\n",
       " 'es': 274,\n",
       " 'Ä b': 275,\n",
       " 'ed': 276,\n",
       " 'Ä f': 277,\n",
       " 'ing': 278,\n",
       " 'Ä p': 279,\n",
       " 'ou': 280,\n",
       " 'Ä an': 281,\n",
       " 'al': 282,\n",
       " 'ar': 283,\n",
       " 'Ä to': 284,\n",
       " 'Ä m': 285,\n",
       " 'Ä of': 286,\n",
       " 'Ä in': 287,\n",
       " 'Ä d': 288,\n",
       " 'Ä h': 289,\n",
       " 'Ä and': 290,\n",
       " 'ic': 291,\n",
       " 'as': 292,\n",
       " 'le': 293,\n",
       " 'Ä th': 294,\n",
       " 'ion': 295,\n",
       " 'om': 296,\n",
       " 'll': 297,\n",
       " 'ent': 298,\n",
       " 'Ä n': 299,\n",
       " 'Ä l': 300,\n",
       " 'st': 301,\n",
       " 'Ä re': 302,\n",
       " 've': 303,\n",
       " 'Ä e': 304,\n",
       " 'ro': 305,\n",
       " 'ly': 306,\n",
       " 'Ä be': 307,\n",
       " 'Ä g': 308,\n",
       " 'Ä T': 309,\n",
       " 'ct': 310,\n",
       " 'Ä S': 311,\n",
       " 'id': 312,\n",
       " 'ot': 313,\n",
       " 'Ä I': 314,\n",
       " 'ut': 315,\n",
       " 'et': 316,\n",
       " 'Ä A': 317,\n",
       " 'Ä is': 318,\n",
       " 'Ä on': 319,\n",
       " 'im': 320,\n",
       " 'am': 321,\n",
       " 'ow': 322,\n",
       " 'ay': 323,\n",
       " 'ad': 324,\n",
       " 'se': 325,\n",
       " 'Ä that': 326,\n",
       " 'Ä C': 327,\n",
       " 'ig': 328,\n",
       " 'Ä for': 329,\n",
       " 'ac': 330,\n",
       " 'Ä y': 331,\n",
       " 'ver': 332,\n",
       " 'ur': 333,\n",
       " 'Ä u': 334,\n",
       " 'ld': 335,\n",
       " 'Ä st': 336,\n",
       " 'Ä M': 337,\n",
       " \"'s\": 338,\n",
       " 'Ä he': 339,\n",
       " 'Ä it': 340,\n",
       " 'ation': 341,\n",
       " 'ith': 342,\n",
       " 'ir': 343,\n",
       " 'ce': 344,\n",
       " 'Ä you': 345,\n",
       " 'il': 346,\n",
       " 'Ä B': 347,\n",
       " 'Ä wh': 348,\n",
       " 'ol': 349,\n",
       " 'Ä P': 350,\n",
       " 'Ä with': 351,\n",
       " 'Ä 1': 352,\n",
       " 'ter': 353,\n",
       " 'ch': 354,\n",
       " 'Ä as': 355,\n",
       " 'Ä we': 356,\n",
       " 'Ä (': 357,\n",
       " 'nd': 358,\n",
       " 'ill': 359,\n",
       " 'Ä D': 360,\n",
       " 'if': 361,\n",
       " 'Ä 2': 362,\n",
       " 'ag': 363,\n",
       " 'ers': 364,\n",
       " 'ke': 365,\n",
       " 'Ä \"': 366,\n",
       " 'Ä H': 367,\n",
       " 'em': 368,\n",
       " 'Ä con': 369,\n",
       " 'Ä W': 370,\n",
       " 'Ä R': 371,\n",
       " 'her': 372,\n",
       " 'Ä was': 373,\n",
       " 'Ä r': 374,\n",
       " 'od': 375,\n",
       " 'Ä F': 376,\n",
       " 'ul': 377,\n",
       " 'ate': 378,\n",
       " 'Ä at': 379,\n",
       " 'ri': 380,\n",
       " 'pp': 381,\n",
       " 'ore': 382,\n",
       " 'Ä The': 383,\n",
       " 'Ä se': 384,\n",
       " 'us': 385,\n",
       " 'Ä pro': 386,\n",
       " 'Ä ha': 387,\n",
       " 'um': 388,\n",
       " 'Ä are': 389,\n",
       " 'Ä de': 390,\n",
       " 'ain': 391,\n",
       " 'and': 392,\n",
       " 'Ä or': 393,\n",
       " 'igh': 394,\n",
       " 'est': 395,\n",
       " 'ist': 396,\n",
       " 'ab': 397,\n",
       " 'rom': 398,\n",
       " 'Ä N': 399,\n",
       " 'th': 400,\n",
       " 'Ä com': 401,\n",
       " 'Ä G': 402,\n",
       " 'un': 403,\n",
       " 'op': 404,\n",
       " '00': 405,\n",
       " 'Ä L': 406,\n",
       " 'Ä not': 407,\n",
       " 'ess': 408,\n",
       " 'Ä ex': 409,\n",
       " 'Ä v': 410,\n",
       " 'res': 411,\n",
       " 'Ä E': 412,\n",
       " 'ew': 413,\n",
       " 'ity': 414,\n",
       " 'ant': 415,\n",
       " 'Ä by': 416,\n",
       " 'el': 417,\n",
       " 'os': 418,\n",
       " 'ort': 419,\n",
       " 'oc': 420,\n",
       " 'qu': 421,\n",
       " 'Ä from': 422,\n",
       " 'Ä have': 423,\n",
       " 'Ä su': 424,\n",
       " 'ive': 425,\n",
       " 'ould': 426,\n",
       " 'Ä sh': 427,\n",
       " 'Ä this': 428,\n",
       " 'nt': 429,\n",
       " 'ra': 430,\n",
       " 'pe': 431,\n",
       " 'ight': 432,\n",
       " 'art': 433,\n",
       " 'ment': 434,\n",
       " 'Ä al': 435,\n",
       " 'ust': 436,\n",
       " 'end': 437,\n",
       " '--': 438,\n",
       " 'all': 439,\n",
       " 'Ä O': 440,\n",
       " 'ack': 441,\n",
       " 'Ä ch': 442,\n",
       " 'Ä le': 443,\n",
       " 'ies': 444,\n",
       " 'red': 445,\n",
       " 'ard': 446,\n",
       " 'Ã¢Ä¢': 447,\n",
       " 'out': 448,\n",
       " 'Ä J': 449,\n",
       " 'Ä ab': 450,\n",
       " 'ear': 451,\n",
       " 'iv': 452,\n",
       " 'ally': 453,\n",
       " 'our': 454,\n",
       " 'ost': 455,\n",
       " 'gh': 456,\n",
       " 'pt': 457,\n",
       " 'Ä pl': 458,\n",
       " 'ast': 459,\n",
       " 'Ä can': 460,\n",
       " 'ak': 461,\n",
       " 'ome': 462,\n",
       " 'ud': 463,\n",
       " 'The': 464,\n",
       " 'Ä his': 465,\n",
       " 'Ä do': 466,\n",
       " 'Ä go': 467,\n",
       " 'Ä has': 468,\n",
       " 'ge': 469,\n",
       " \"'t\": 470,\n",
       " 'Ä U': 471,\n",
       " 'rou': 472,\n",
       " 'Ä sa': 473,\n",
       " 'Ä j': 474,\n",
       " 'Ä but': 475,\n",
       " 'Ä wor': 476,\n",
       " 'Ä all': 477,\n",
       " 'ect': 478,\n",
       " 'Ä k': 479,\n",
       " 'ame': 480,\n",
       " 'Ä will': 481,\n",
       " 'ok': 482,\n",
       " 'Ä whe': 483,\n",
       " 'Ä they': 484,\n",
       " 'ide': 485,\n",
       " '01': 486,\n",
       " 'ff': 487,\n",
       " 'ich': 488,\n",
       " 'pl': 489,\n",
       " 'ther': 490,\n",
       " 'Ä tr': 491,\n",
       " '..': 492,\n",
       " 'Ä int': 493,\n",
       " 'ie': 494,\n",
       " 'ure': 495,\n",
       " 'age': 496,\n",
       " 'Ä ne': 497,\n",
       " 'ial': 498,\n",
       " 'ap': 499,\n",
       " 'ine': 500,\n",
       " 'ice': 501,\n",
       " 'Ä me': 502,\n",
       " 'Ä out': 503,\n",
       " 'ans': 504,\n",
       " 'one': 505,\n",
       " 'ong': 506,\n",
       " 'ions': 507,\n",
       " 'Ä who': 508,\n",
       " 'Ä K': 509,\n",
       " 'Ä up': 510,\n",
       " 'Ä their': 511,\n",
       " 'Ä ad': 512,\n",
       " 'Ä 3': 513,\n",
       " 'Ä us': 514,\n",
       " 'ated': 515,\n",
       " 'ous': 516,\n",
       " 'Ä more': 517,\n",
       " 'ue': 518,\n",
       " 'og': 519,\n",
       " 'Ä St': 520,\n",
       " 'ind': 521,\n",
       " 'ike': 522,\n",
       " 'Ä so': 523,\n",
       " 'ime': 524,\n",
       " 'per': 525,\n",
       " '.\"': 526,\n",
       " 'ber': 527,\n",
       " 'iz': 528,\n",
       " 'act': 529,\n",
       " 'Ä one': 530,\n",
       " 'Ä said': 531,\n",
       " 'Ä -': 532,\n",
       " 'are': 533,\n",
       " 'Ä your': 534,\n",
       " 'cc': 535,\n",
       " 'Ä Th': 536,\n",
       " 'Ä cl': 537,\n",
       " 'ep': 538,\n",
       " 'ake': 539,\n",
       " 'able': 540,\n",
       " 'ip': 541,\n",
       " 'Ä cont': 542,\n",
       " 'Ä which': 543,\n",
       " 'ia': 544,\n",
       " 'Ä im': 545,\n",
       " 'Ä about': 546,\n",
       " 'Ä were': 547,\n",
       " 'very': 548,\n",
       " 'ub': 549,\n",
       " 'Ä had': 550,\n",
       " 'Ä en': 551,\n",
       " 'Ä comp': 552,\n",
       " ',\"': 553,\n",
       " 'Ä In': 554,\n",
       " 'Ä un': 555,\n",
       " 'Ä ag': 556,\n",
       " 'ire': 557,\n",
       " 'ace': 558,\n",
       " 'au': 559,\n",
       " 'ary': 560,\n",
       " 'Ä would': 561,\n",
       " 'ass': 562,\n",
       " 'ry': 563,\n",
       " 'Ä Ã¢Ä¢': 564,\n",
       " 'cl': 565,\n",
       " 'ook': 566,\n",
       " 'ere': 567,\n",
       " 'so': 568,\n",
       " 'Ä V': 569,\n",
       " 'ign': 570,\n",
       " 'ib': 571,\n",
       " 'Ä off': 572,\n",
       " 'Ä te': 573,\n",
       " 'ven': 574,\n",
       " 'Ä Y': 575,\n",
       " 'ile': 576,\n",
       " 'ose': 577,\n",
       " 'ite': 578,\n",
       " 'orm': 579,\n",
       " 'Ä 201': 580,\n",
       " 'Ä res': 581,\n",
       " 'Ä man': 582,\n",
       " 'Ä per': 583,\n",
       " 'Ä other': 584,\n",
       " 'ord': 585,\n",
       " 'ult': 586,\n",
       " 'Ä been': 587,\n",
       " 'Ä like': 588,\n",
       " 'ase': 589,\n",
       " 'ance': 590,\n",
       " 'ks': 591,\n",
       " 'ays': 592,\n",
       " 'own': 593,\n",
       " 'ence': 594,\n",
       " 'Ä dis': 595,\n",
       " 'ction': 596,\n",
       " 'Ä any': 597,\n",
       " 'Ä app': 598,\n",
       " 'Ä sp': 599,\n",
       " 'int': 600,\n",
       " 'ress': 601,\n",
       " 'ations': 602,\n",
       " 'ail': 603,\n",
       " 'Ä 4': 604,\n",
       " 'ical': 605,\n",
       " 'Ä them': 606,\n",
       " 'Ä her': 607,\n",
       " 'ount': 608,\n",
       " 'Ä Ch': 609,\n",
       " 'Ä ar': 610,\n",
       " 'Ä if': 611,\n",
       " 'Ä there': 612,\n",
       " 'Ä pe': 613,\n",
       " 'Ä year': 614,\n",
       " 'av': 615,\n",
       " 'Ä my': 616,\n",
       " 'Ä some': 617,\n",
       " 'Ä when': 618,\n",
       " 'ough': 619,\n",
       " 'ach': 620,\n",
       " 'Ä than': 621,\n",
       " 'ru': 622,\n",
       " 'ond': 623,\n",
       " 'ick': 624,\n",
       " 'Ä over': 625,\n",
       " 'vel': 626,\n",
       " 'Ä qu': 627,\n",
       " 'ÄŠÄŠ': 628,\n",
       " 'Ä sc': 629,\n",
       " 'reat': 630,\n",
       " 'ree': 631,\n",
       " 'Ä It': 632,\n",
       " 'ound': 633,\n",
       " 'port': 634,\n",
       " 'Ä also': 635,\n",
       " 'Ä part': 636,\n",
       " 'fter': 637,\n",
       " 'Ä kn': 638,\n",
       " 'Ä bec': 639,\n",
       " 'Ä time': 640,\n",
       " 'ens': 641,\n",
       " 'Ä 5': 642,\n",
       " 'ople': 643,\n",
       " 'Ä what': 644,\n",
       " 'Ä no': 645,\n",
       " 'du': 646,\n",
       " 'mer': 647,\n",
       " 'ang': 648,\n",
       " 'Ä new': 649,\n",
       " '----': 650,\n",
       " 'Ä get': 651,\n",
       " 'ory': 652,\n",
       " 'ition': 653,\n",
       " 'ings': 654,\n",
       " 'Ä just': 655,\n",
       " 'Ä into': 656,\n",
       " 'Ä 0': 657,\n",
       " 'ents': 658,\n",
       " 'ove': 659,\n",
       " 'te': 660,\n",
       " 'Ä people': 661,\n",
       " 'Ä pre': 662,\n",
       " 'Ä its': 663,\n",
       " 'Ä rec': 664,\n",
       " 'Ä tw': 665,\n",
       " 'ian': 666,\n",
       " 'irst': 667,\n",
       " 'ark': 668,\n",
       " 'ors': 669,\n",
       " 'Ä work': 670,\n",
       " 'ade': 671,\n",
       " 'ob': 672,\n",
       " 'Ä she': 673,\n",
       " 'Ä our': 674,\n",
       " 'wn': 675,\n",
       " 'ink': 676,\n",
       " 'lic': 677,\n",
       " 'Ä 19': 678,\n",
       " 'Ä He': 679,\n",
       " 'ish': 680,\n",
       " 'nder': 681,\n",
       " 'ause': 682,\n",
       " 'Ä him': 683,\n",
       " 'ons': 684,\n",
       " 'Ä [': 685,\n",
       " 'Ä ro': 686,\n",
       " 'form': 687,\n",
       " 'ild': 688,\n",
       " 'ates': 689,\n",
       " 'vers': 690,\n",
       " 'Ä only': 691,\n",
       " 'oll': 692,\n",
       " 'Ä spe': 693,\n",
       " 'ck': 694,\n",
       " 'ell': 695,\n",
       " 'amp': 696,\n",
       " 'Ä acc': 697,\n",
       " 'Ä bl': 698,\n",
       " 'ious': 699,\n",
       " 'urn': 700,\n",
       " 'ft': 701,\n",
       " 'ood': 702,\n",
       " 'Ä how': 703,\n",
       " 'hed': 704,\n",
       " \"Ä '\": 705,\n",
       " 'Ä after': 706,\n",
       " 'aw': 707,\n",
       " 'Ä att': 708,\n",
       " 'ov': 709,\n",
       " 'ne': 710,\n",
       " 'Ä play': 711,\n",
       " 'erv': 712,\n",
       " 'ict': 713,\n",
       " 'Ä could': 714,\n",
       " 'itt': 715,\n",
       " 'Ä am': 716,\n",
       " 'Ä first': 717,\n",
       " 'Ä 6': 718,\n",
       " 'Ä act': 719,\n",
       " 'Ä $': 720,\n",
       " 'ec': 721,\n",
       " 'hing': 722,\n",
       " 'ual': 723,\n",
       " 'ull': 724,\n",
       " 'Ä comm': 725,\n",
       " 'oy': 726,\n",
       " 'old': 727,\n",
       " 'ces': 728,\n",
       " 'ater': 729,\n",
       " 'Ä fe': 730,\n",
       " 'Ä bet': 731,\n",
       " 'we': 732,\n",
       " 'iff': 733,\n",
       " 'Ä two': 734,\n",
       " 'ock': 735,\n",
       " 'Ä back': 736,\n",
       " ').': 737,\n",
       " 'ident': 738,\n",
       " 'Ä under': 739,\n",
       " 'rough': 740,\n",
       " 'sel': 741,\n",
       " 'xt': 742,\n",
       " 'Ä may': 743,\n",
       " 'round': 744,\n",
       " 'Ä po': 745,\n",
       " 'ph': 746,\n",
       " 'iss': 747,\n",
       " 'Ä des': 748,\n",
       " 'Ä most': 749,\n",
       " 'Ä did': 750,\n",
       " 'Ä add': 751,\n",
       " 'ject': 752,\n",
       " 'Ä inc': 753,\n",
       " 'fore': 754,\n",
       " 'Ä pol': 755,\n",
       " 'ont': 756,\n",
       " 'Ä again': 757,\n",
       " 'clud': 758,\n",
       " 'tern': 759,\n",
       " 'Ä know': 760,\n",
       " 'Ä need': 761,\n",
       " 'Ä cons': 762,\n",
       " 'Ä co': 763,\n",
       " 'Ä .': 764,\n",
       " 'Ä want': 765,\n",
       " 'Ä see': 766,\n",
       " 'Ä 7': 767,\n",
       " 'ning': 768,\n",
       " 'iew': 769,\n",
       " 'Ä This': 770,\n",
       " 'ced': 771,\n",
       " 'Ä even': 772,\n",
       " 'Ä ind': 773,\n",
       " 'ty': 774,\n",
       " 'Ä We': 775,\n",
       " 'ath': 776,\n",
       " 'Ä these': 777,\n",
       " 'Ä pr': 778,\n",
       " 'Ä use': 779,\n",
       " 'Ä because': 780,\n",
       " 'Ä fl': 781,\n",
       " 'ng': 782,\n",
       " 'Ä now': 783,\n",
       " 'Ä Ã¢Ä¢Äµ': 784,\n",
       " 'com': 785,\n",
       " 'ise': 786,\n",
       " 'Ä make': 787,\n",
       " 'Ä then': 788,\n",
       " 'ower': 789,\n",
       " 'Ä every': 790,\n",
       " 'Ä Un': 791,\n",
       " 'Ä sec': 792,\n",
       " 'oss': 793,\n",
       " 'uch': 794,\n",
       " 'Ä em': 795,\n",
       " 'Ä =': 796,\n",
       " 'Ä Re': 797,\n",
       " 'ied': 798,\n",
       " 'rit': 799,\n",
       " 'Ä inv': 800,\n",
       " 'lect': 801,\n",
       " 'Ä supp': 802,\n",
       " 'ating': 803,\n",
       " 'Ä look': 804,\n",
       " 'man': 805,\n",
       " 'pect': 806,\n",
       " 'Ä 8': 807,\n",
       " 'row': 808,\n",
       " 'Ä bu': 809,\n",
       " 'Ä where': 810,\n",
       " 'ific': 811,\n",
       " 'Ä years': 812,\n",
       " 'ily': 813,\n",
       " 'Ä diff': 814,\n",
       " 'Ä should': 815,\n",
       " 'Ä rem': 816,\n",
       " 'Th': 817,\n",
       " 'In': 818,\n",
       " 'Ä ev': 819,\n",
       " 'day': 820,\n",
       " \"'re\": 821,\n",
       " 'rib': 822,\n",
       " 'Ä rel': 823,\n",
       " 'ss': 824,\n",
       " 'Ä def': 825,\n",
       " 'Ä right': 826,\n",
       " 'Ä sy': 827,\n",
       " '),': 828,\n",
       " 'les': 829,\n",
       " '000': 830,\n",
       " 'hen': 831,\n",
       " 'Ä through': 832,\n",
       " 'Ä Tr': 833,\n",
       " '__': 834,\n",
       " 'Ä way': 835,\n",
       " 'Ä don': 836,\n",
       " 'Ä ,': 837,\n",
       " 'Ä 10': 838,\n",
       " 'ased': 839,\n",
       " 'Ä ass': 840,\n",
       " 'ublic': 841,\n",
       " 'Ä reg': 842,\n",
       " 'Ä And': 843,\n",
       " 'ix': 844,\n",
       " 'Ä very': 845,\n",
       " 'Ä includ': 846,\n",
       " 'other': 847,\n",
       " 'Ä imp': 848,\n",
       " 'oth': 849,\n",
       " 'Ä sub': 850,\n",
       " 'Ä Ã¢Ä¢Ä¶': 851,\n",
       " 'Ä being': 852,\n",
       " 'arg': 853,\n",
       " 'Ä Wh': 854,\n",
       " '==': 855,\n",
       " 'ible': 856,\n",
       " 'Ä does': 857,\n",
       " 'ange': 858,\n",
       " 'ram': 859,\n",
       " 'Ä 9': 860,\n",
       " 'ert': 861,\n",
       " 'ps': 862,\n",
       " 'ited': 863,\n",
       " 'ational': 864,\n",
       " 'Ä br': 865,\n",
       " 'Ä down': 866,\n",
       " 'Ä many': 867,\n",
       " 'aking': 868,\n",
       " 'Ä call': 869,\n",
       " 'uring': 870,\n",
       " 'ities': 871,\n",
       " 'Ä ph': 872,\n",
       " 'ics': 873,\n",
       " 'als': 874,\n",
       " 'Ä dec': 875,\n",
       " 'ative': 876,\n",
       " 'ener': 877,\n",
       " 'Ä before': 878,\n",
       " 'ility': 879,\n",
       " 'Ä well': 880,\n",
       " 'Ä much': 881,\n",
       " 'erson': 882,\n",
       " 'Ä those': 883,\n",
       " 'Ä such': 884,\n",
       " 'Ä ke': 885,\n",
       " 'Ä end': 886,\n",
       " 'Ä But': 887,\n",
       " 'ason': 888,\n",
       " 'ting': 889,\n",
       " 'Ä long': 890,\n",
       " 'ef': 891,\n",
       " 'Ä think': 892,\n",
       " 'ys': 893,\n",
       " 'Ä bel': 894,\n",
       " 'Ä sm': 895,\n",
       " 'its': 896,\n",
       " 'ax': 897,\n",
       " 'Ä own': 898,\n",
       " 'Ä prov': 899,\n",
       " 'Ä set': 900,\n",
       " 'ife': 901,\n",
       " 'ments': 902,\n",
       " 'ble': 903,\n",
       " 'ward': 904,\n",
       " 'Ä show': 905,\n",
       " 'Ä pres': 906,\n",
       " 'ms': 907,\n",
       " 'omet': 908,\n",
       " 'Ä ob': 909,\n",
       " 'Ä say': 910,\n",
       " 'Ä Sh': 911,\n",
       " 'ts': 912,\n",
       " 'ful': 913,\n",
       " 'Ä eff': 914,\n",
       " 'Ä gu': 915,\n",
       " 'Ä inst': 916,\n",
       " 'und': 917,\n",
       " 'ren': 918,\n",
       " 'cess': 919,\n",
       " 'Ä ent': 920,\n",
       " 'Ä You': 921,\n",
       " 'Ä good': 922,\n",
       " 'Ä start': 923,\n",
       " 'ince': 924,\n",
       " 'Ä made': 925,\n",
       " 'tt': 926,\n",
       " 'stem': 927,\n",
       " 'olog': 928,\n",
       " 'up': 929,\n",
       " 'Ä |': 930,\n",
       " 'ump': 931,\n",
       " 'Ä hel': 932,\n",
       " 'vern': 933,\n",
       " 'ular': 934,\n",
       " 'ually': 935,\n",
       " 'Ä ac': 936,\n",
       " 'Ä mon': 937,\n",
       " 'Ä last': 938,\n",
       " 'Ä 200': 939,\n",
       " '10': 940,\n",
       " 'Ä stud': 941,\n",
       " 'ures': 942,\n",
       " 'Ä Ar': 943,\n",
       " 'self': 944,\n",
       " 'ars': 945,\n",
       " 'meric': 946,\n",
       " 'ues': 947,\n",
       " 'cy': 948,\n",
       " 'Ä min': 949,\n",
       " 'ollow': 950,\n",
       " 'Ä col': 951,\n",
       " 'io': 952,\n",
       " 'Ä mod': 953,\n",
       " 'Ä count': 954,\n",
       " 'Ä Com': 955,\n",
       " 'hes': 956,\n",
       " 'Ä fin': 957,\n",
       " 'air': 958,\n",
       " 'ier': 959,\n",
       " 'Ã¢Ä¢Ä¶': 960,\n",
       " 'read': 961,\n",
       " 'ank': 962,\n",
       " 'atch': 963,\n",
       " 'ever': 964,\n",
       " 'Ä str': 965,\n",
       " 'Ä point': 966,\n",
       " 'ork': 967,\n",
       " 'Ä New': 968,\n",
       " 'Ä sur': 969,\n",
       " 'ool': 970,\n",
       " 'alk': 971,\n",
       " 'ement': 972,\n",
       " 'Ä used': 973,\n",
       " 'ract': 974,\n",
       " 'ween': 975,\n",
       " 'Ä same': 976,\n",
       " 'oun': 977,\n",
       " 'Ä Al': 978,\n",
       " 'ci': 979,\n",
       " 'Ä differe': 980,\n",
       " 'Ä while': 981,\n",
       " '--------': 982,\n",
       " 'Ä game': 983,\n",
       " 'cept': 984,\n",
       " 'Ä sim': 985,\n",
       " '...': 986,\n",
       " 'Ä inter': 987,\n",
       " 'ek': 988,\n",
       " 'Ä report': 989,\n",
       " 'Ä produ': 990,\n",
       " 'Ä still': 991,\n",
       " 'led': 992,\n",
       " 'ah': 993,\n",
       " 'Ä here': 994,\n",
       " 'Ä world': 995,\n",
       " 'Ä though': 996,\n",
       " 'Ä num': 997,\n",
       " 'arch': 998,\n",
       " 'imes': 999,\n",
       " ...}"
      ]
     },
     "execution_count": 100,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.get_vocab()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "beef3eb8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[15496, 11, 12520, 234, 235, 0, 220, 19526, 254, 25001, 121, 0]\n"
     ]
    }
   ],
   "source": [
    "string = \"Hello, ðŸŒ! ä½ å¥½!\"\n",
    "print(tokenizer.encode(string))\n",
    "tokens = tokenizer.encode(string)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "2ebdad81",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "15496 ->  Hello\n",
      "11 ->  ,\n",
      "12520 ->   ï¿½\n",
      "234 ->  ï¿½\n",
      "235 ->  ï¿½\n",
      "0 ->  !\n",
      "220 ->   \n",
      "19526 ->  ï¿½\n",
      "254 ->  ï¿½\n",
      "25001 ->  ï¿½\n",
      "121 ->  ï¿½\n",
      "0 ->  !\n"
     ]
    }
   ],
   "source": [
    "# decode single value\n",
    "for token in tokens:\n",
    "    print(f\"{token} -> \", tokenizer.decode(token))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "a8f78a7c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compression_ratio(string:str, tokens:list):\n",
    "    original_size = len(bytes(string, encoding=\"utf-8\"))\n",
    "    num_tokens = len(tokens)\n",
    "    return original_size/num_tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "c8b310fb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "original_size ->  20\n",
      "num_tokens ->  12\n",
      "compression ->  1.6666666666666667\n",
      "compression_formula ->  1.6666666666666667\n"
     ]
    }
   ],
   "source": [
    "# compression ratio\n",
    "original_size = len(bytes(string, encoding=\"utf-8\"))\n",
    "num_tokens = len(tokens)\n",
    "\n",
    "\n",
    "print(\"original_size -> \", original_size)\n",
    "print(\"num_tokens -> \", num_tokens)\n",
    "print(\"compression -> \", original_size/num_tokens)\n",
    "print(\"compression_formula -> \", compression_ratio(string=string, tokens=tokens))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "6b68db19",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "H\n",
      "e\n",
      "l\n",
      "l\n",
      "o\n",
      ",\n",
      " \n",
      "ðŸŒ\n",
      "!\n",
      " \n",
      "ä½ \n",
      "å¥½\n",
      "!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "({'H': 0,\n",
       "  'e': 1,\n",
       "  'l': 2,\n",
       "  'o': 3,\n",
       "  ',': 4,\n",
       "  ' ': 5,\n",
       "  'ðŸŒ': 6,\n",
       "  '!': 7,\n",
       "  'ä½ ': 8,\n",
       "  'å¥½': 9},\n",
       " [0, 1, 2, 2, 3, 4, 5, 6, 7, 5, 8, 9, 7])"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Options for tokenizes\n",
    "# Option 1: Each character a token\n",
    "# Lets then map each character to a number\n",
    "# do I need catalog? \n",
    "\n",
    "vocab_dict = {}\n",
    "index_cnt = 0\n",
    "char_tokens = []\n",
    "for char in string: \n",
    "    print(char)\n",
    "    if char not in vocab_dict:\n",
    "        vocab_dict[char] = index_cnt\n",
    "        char_tokens.append(index_cnt)\n",
    "        index_cnt += 1\n",
    "\n",
    "    else:\n",
    "        char_tokens.append(vocab_dict[char])\n",
    "\n",
    "#tokens\n",
    "vocab_dict, char_tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "b980ddff",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "char compression -> ,  1.5384615384615385\n"
     ]
    }
   ],
   "source": [
    "print(\"char compression -> , \", compression_ratio(string=string, tokens=char_tokens))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "14470bb2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "97"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ord(\"a\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "af02d31b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'a'"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chr(97)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "5346ee6e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "b'a'\n",
      "b'\\xf0\\x9f\\x8c\\x8e'\n"
     ]
    }
   ],
   "source": [
    "# Byte-based tokenization\n",
    "# unicode strings can be represented as integers between 0 and 255\n",
    "# utf-8 representation\n",
    "print(bytes(\"a\", encoding=\"utf-8\"))\n",
    "print(bytes('ðŸŒŽ', encoding=\"utf-8\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "96505f40",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "240"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "int.from_bytes(b'\\xf0')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "ed30dcbf",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(bytes('ðŸŒŽ', encoding=\"utf-8\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "d86c432a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "byte tokens:  [72, 101, 108, 108, 111, 44, 32, 240, 159, 140, 141, 33, 32, 228, 189, 160, 229, 165, 189, 33]\n"
     ]
    }
   ],
   "source": [
    "def byte_char_encoding(string:str):\n",
    "    \"\"\" Splits string in separate characters\n",
    "     Split in single bytes, case a character is represented by more than 1 byte. \n",
    "       \"\"\"\n",
    "    vocab_dict = {}\n",
    "\n",
    "    char_tokens = []\n",
    "    for char in string: \n",
    "        #print(bytes(char, encoding=\"utf-8\"))\n",
    "        char_bytes = bytes(char, encoding=\"utf-8\")\n",
    "        if len(char_bytes) == 0:\n",
    "            byte = char_bytes[0]\n",
    "            vocab_dict[char] = char_bytes\n",
    "            char_tokens.append(byte)\n",
    "\n",
    "        else: \n",
    "            vocab_dict[char] = char_bytes\n",
    "\n",
    "            for b in char_bytes:\n",
    "                char_tokens.append(b)\n",
    "\n",
    "    return char_tokens\n",
    "\n",
    "\n",
    "byte_tokens = byte_char_encoding(string=string)\n",
    "print(\"byte tokens: \", byte_tokens)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "1432cce6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "byter compression -> ,  1.0\n"
     ]
    }
   ],
   "source": [
    "print(\"byter compression -> , \", compression_ratio(string=string, \n",
    "                                                   tokens=byte_tokens))\n",
    "\n",
    "# is 1, which is the worst possible compression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "da84acb6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'H'"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chr(72)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "45a96050",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Word based tokenization\n",
    "# 1st define regez to split the sentence\n",
    "# Then split\n",
    "WORD_REGEX = r'([, !?])'  # Lets use just space for now\n",
    "\n",
    "import re\n",
    "sentence_word = re.split(pattern=WORD_REGEX, string=string)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "33c4d615",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hello, ðŸŒ! ä½ å¥½! \n",
      " ['Hello', ',', '', ' ', 'ðŸŒ', '!', '', ' ', 'ä½ å¥½', '!', '']\n"
     ]
    }
   ],
   "source": [
    "print(string,\"\\n\", sentence_word) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "cb838ac2",
   "metadata": {},
   "outputs": [],
   "source": [
    "clean_sentence = [word for word in sentence_word if word != \"\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "0595a68c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Hello', ',', ' ', 'ðŸŒ', '!', ' ', 'ä½ å¥½', '!']"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clean_sentence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "15f4f127",
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab = {}\n",
    "index = 0\n",
    "word_tokens = []\n",
    "for word in clean_sentence:\n",
    "    if word not in vocab: \n",
    "        vocab[word] = index\n",
    "        word_tokens.append(index)\n",
    "        index += 1\n",
    "    else:\n",
    "        word_tokens.append(vocab[word])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "b0b96a27",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(['Hello', ',', ' ', 'ðŸŒ', '!', ' ', 'ä½ å¥½', '!'], 'Hello, ðŸŒ! ä½ å¥½!')"
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clean_sentence, string"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "5ee7cba5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0, 1, 2, 3, 4, 2, 5, 4]"
      ]
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "word_tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "c7750376",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2.5"
      ]
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "compression_ratio(string=string, tokens=word_tokens)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ed9debaa",
   "metadata": {},
   "source": [
    "# Byte Pair Encoding (BPE)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "0069b807",
   "metadata": {},
   "outputs": [],
   "source": [
    "#### Step 1: Convert each char into a single byte\n",
    "#### Step 2: Count each adjacent pair\n",
    "#### Step 3: Merge the top1 most frequent\n",
    "#### Step 4: Repeat with a fix limit of merges ??? \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "52169601",
   "metadata": {},
   "outputs": [],
   "source": [
    "sentence = \"the cat is in the hat\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "4771f5a3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "127758"
      ]
     },
     "execution_count": 68,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bytes('ðŸŒŽ', encoding=\"utf-8\")\n",
    "ord('ðŸŒŽ')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "02a6f25c",
   "metadata": {},
   "outputs": [],
   "source": [
    "char_bytes = [ord(char) for char in sentence]\n",
    "\n",
    "counts_dict = {}\n",
    "for i in range(len(char_bytes)-1):\n",
    "    pair =  (char_bytes[i],char_bytes[i+1])\n",
    "    if pair not in counts_dict:\n",
    "        counts_dict[pair] = 0\n",
    "    else: \n",
    "        counts_dict[pair] += 1\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "71d9c9d9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{(116, 104): 1, (104, 101): 1, (101, 32): 1, (97, 116): 1, (32, 105): 1, (32, 99): 0, (99, 97): 0, (116, 32): 0, (105, 115): 0, (115, 32): 0, (105, 110): 0, (110, 32): 0, (32, 116): 0, (32, 104): 0, (104, 97): 0}\n"
     ]
    }
   ],
   "source": [
    "counts_dict = {k: v for k, v in sorted(counts_dict.items(), key=lambda item: item[1], reverse=True)}\n",
    "print(counts_dict)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "428647ee",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{(116, 104): 1,\n",
       " (104, 101): 1,\n",
       " (101, 32): 1,\n",
       " (97, 116): 1,\n",
       " (32, 105): 1,\n",
       " (32, 99): 0,\n",
       " (99, 97): 0,\n",
       " (116, 32): 0,\n",
       " (105, 115): 0,\n",
       " (115, 32): 0,\n",
       " (105, 110): 0,\n",
       " (110, 32): 0,\n",
       " (32, 116): 0,\n",
       " (32, 104): 0,\n",
       " (104, 97): 0}"
      ]
     },
     "execution_count": 71,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "counts_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "9a894a1f",
   "metadata": {},
   "outputs": [],
   "source": [
    "to_merge = next(iter(counts_dict))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "05c22f82",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(116, 104)"
      ]
     },
     "execution_count": 73,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "to_merge"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "c87eec62",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "104"
      ]
     },
     "execution_count": 74,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# merge\n",
    "next_token = 257\n",
    "merge_position = char_bytes.index(to_merge[0])\n",
    "\n",
    "char_bytes.pop(merge_position)\n",
    "# repeat since the list will move left \n",
    "char_bytes.pop(merge_position)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "id": "79c6c583",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[101,\n",
       " 32,\n",
       " 99,\n",
       " 97,\n",
       " 116,\n",
       " 32,\n",
       " 105,\n",
       " 115,\n",
       " 32,\n",
       " 105,\n",
       " 110,\n",
       " 32,\n",
       " 116,\n",
       " 104,\n",
       " 101,\n",
       " 32,\n",
       " 104,\n",
       " 97,\n",
       " 116]"
      ]
     },
     "execution_count": 75,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "char_bytes "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "id": "91705dbe",
   "metadata": {},
   "outputs": [],
   "source": [
    "# insert new token on the removed indexes\n",
    "char_bytes.insert(merge_position, next_token)\n",
    "next_token += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "id": "c389fb70",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[257,\n",
       " 101,\n",
       " 32,\n",
       " 99,\n",
       " 97,\n",
       " 116,\n",
       " 32,\n",
       " 105,\n",
       " 115,\n",
       " 32,\n",
       " 105,\n",
       " 110,\n",
       " 32,\n",
       " 116,\n",
       " 104,\n",
       " 101,\n",
       " 32,\n",
       " 104,\n",
       " 97,\n",
       " 116]"
      ]
     },
     "execution_count": 77,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "char_bytes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4aca2042",
   "metadata": {},
   "outputs": [],
   "source": [
    "#### Step 1: Convert each char into a single byte\n",
    "\n",
    "class BytePairEncoder:\n",
    "    \n",
    "    def __init__(self, num_merges:int=10):\n",
    "        self.num_merges = num_merges\n",
    "        self.vocab = {}\n",
    "        self.token_dict = {}\n",
    "\n",
    "    def train(self, sentence:str) -> dict:\n",
    "        \"\"\" Output is dict with vocabolary like \n",
    "        \n",
    "        {\n",
    "           'a' : 1\n",
    "           '!' : 2\n",
    "        }\n",
    "        \"\"\"\n",
    "        char_dict = {char:ord(char) for idx, char in enumerate(sentence)}\n",
    "        self.vocab = char_dict\n",
    "        return char_dict\n",
    "\n",
    "    \n",
    "    def encode(self, sentence:str):\n",
    "        NEXT_TOKEN = 256\n",
    "        char_bytes = [ord(char) for char in sentence]\n",
    "        for _ in range(self.num_merges):\n",
    "            new_chars = []\n",
    "\n",
    "            # pairs counter\n",
    "            counts_dict = {}\n",
    "            for i in range(len(char_bytes)-1):\n",
    "                pair =  (char_bytes[i],char_bytes[i+1])\n",
    "                if pair not in counts_dict:\n",
    "                    counts_dict[pair] = 1\n",
    "                else: \n",
    "                    counts_dict[pair] += 1\n",
    "            \n",
    "\n",
    "            # order\n",
    "            counts_dict = {k: v for k, v in sorted(counts_dict.items(), key=lambda item: item[1], reverse=True)}\n",
    "\n",
    "            # find pair to merge\n",
    "            to_merge_key = next(iter(counts_dict))\n",
    "            print(\"to merge:  \", to_merge_key)\n",
    "\n",
    "            \n",
    "            # remove merged\n",
    "            pointer = 0\n",
    "            while pointer < len(char_bytes)-1: \n",
    "                if char_bytes[pointer] ==  to_merge_key[0] and char_bytes[pointer + 1] == to_merge_key[1]:\n",
    "                    new_chars.append(NEXT_TOKEN)\n",
    "                    pointer += 2\n",
    "\n",
    "                else: \n",
    "                    new_chars.append(char_bytes[pointer])\n",
    "                    pointer += 1\n",
    "            NEXT_TOKEN += 1\n",
    "            if pointer != len(char_bytes):\n",
    "                new_chars.append(char_bytes[pointer])\n",
    "            \n",
    "            char_bytes = new_chars\n",
    "            \n",
    "        return char_bytes\n",
    "\n",
    "\n",
    "    def decode(self, tokens_list:list[int]) -> str:\n",
    "        \"\"\" uses the vocab to  convert back to sentence \"\"\"\n",
    "        final_string = []\n",
    "        \n",
    "        # reverse vocab\n",
    "        reverse_vocab = {v:k for k,v in self.vocab.items()}\n",
    "        \n",
    "        for token in tokens_list:\n",
    "            final_string.append(reverse_vocab[token])\n",
    "        \n",
    "        return \"\".join(final_string) \n",
    "    \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "id": "4c82b6a5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'t': 116, 'h': 104, 'e': 101, ' ': 32, 'c': 99, 'a': 97, 'i': 105, 's': 115, 'n': 110}\n"
     ]
    }
   ],
   "source": [
    "bpe = BytePairEncoder(num_merges=1)\n",
    "\n",
    "print(bpe.train(sentence=sentence))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "id": "8d6e919d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'t': 116,\n",
       " 'h': 104,\n",
       " 'e': 101,\n",
       " ' ': 32,\n",
       " 'c': 99,\n",
       " 'a': 97,\n",
       " 'i': 105,\n",
       " 's': 115,\n",
       " 'n': 110}"
      ]
     },
     "execution_count": 108,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bpe.vocab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "id": "0fc195cf",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "20"
      ]
     },
     "execution_count": 103,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(char_bytes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "id": "3e208ea1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'the cat is in the hat'"
      ]
     },
     "execution_count": 80,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sentence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "id": "6c9dc383",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[256, 101, 32, 99, 97, 116, 32, 105, 115, 32, 105, 110, 32, 256, 101, 32, 104, 97, 116]\n"
     ]
    }
   ],
   "source": [
    "from bpetokenizer import BPETokenizer\n",
    "tokenizer = BPETokenizer()\n",
    "tokenizer.train(sentence, vocab_size=257)\n",
    "print(tokenizer.encode(sentence))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "id": "dd966116",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "to merge:   (116, 104)\n",
      "[256, 101, 32, 99, 97, 116, 32, 105, 115, 32, 105, 110, 32, 256, 101, 32, 104, 97, 116] 19\n",
      "to merge:   (116, 104)\n",
      "to merge:   (256, 101)\n",
      "[257, 32, 99, 97, 116, 32, 105, 115, 32, 105, 110, 32, 257, 32, 104, 97, 116] 17\n",
      "to merge:   (116, 104)\n",
      "to merge:   (256, 101)\n",
      "to merge:   (257, 32)\n",
      "[258, 99, 97, 116, 32, 105, 115, 32, 105, 110, 32, 258, 104, 97, 116] 15\n"
     ]
    }
   ],
   "source": [
    "bpe = BytePairEncoder(num_merges=1)\n",
    "result = bpe.encode(sentence=sentence)\n",
    "print(result, len(result))\n",
    "bpe = BytePairEncoder(num_merges=2)\n",
    "result = bpe.encode(sentence=sentence)\n",
    "print(result, len(result))\n",
    "\n",
    "bpe = BytePairEncoder(num_merges=3)\n",
    "result = bpe.encode(sentence=sentence)\n",
    "print(result, len(result))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "id": "08eebecf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "to merge:   (116, 104)\n",
      "[256, 101, 32, 99, 97, 116, 32, 105, 115, 32, 105, 110, 32, 256, 101, 32, 104, 97, 116]\n",
      "[256, 101, 32, 99, 97, 116, 32, 105, 115, 32, 105, 110, 32, 256, 101, 32, 104, 97, 116]\n"
     ]
    }
   ],
   "source": [
    "print(bpe.encode(sentence=sentence))\n",
    "print(tokenizer.encode(sentence))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2feee059",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "vocab is  257\n",
      "to merge:   (116, 104)\n",
      "my bpe  [256, 101, 32, 99, 97, 116, 32, 105, 115, 32, 105, 110, 32, 256, 101, 32, 104, 97, 116]\n",
      "   lib: [256, 101, 32, 99, 97, 116, 32, 105, 115, 32, 105, 110, 32, 256, 101, 32, 104, 97, 116]\n",
      "vocab is  258\n",
      "to merge:   (116, 104)\n",
      "to merge:   (256, 101)\n",
      "my bpe  [257, 32, 99, 97, 116, 32, 105, 115, 32, 105, 110, 32, 257, 32, 104, 97, 116]\n",
      "   lib: [257, 32, 99, 97, 116, 32, 105, 115, 32, 105, 110, 32, 257, 32, 104, 97, 116]\n",
      "vocab is  259\n",
      "to merge:   (116, 104)\n",
      "to merge:   (256, 101)\n",
      "to merge:   (257, 32)\n",
      "my bpe  [258, 99, 97, 116, 32, 105, 115, 32, 105, 110, 32, 258, 104, 97, 116]\n",
      "   lib: [257, 32, 99, 258, 32, 105, 115, 32, 105, 110, 32, 257, 32, 104, 258]\n",
      "vocab is  260\n",
      "to merge:   (116, 104)\n",
      "to merge:   (256, 101)\n",
      "to merge:   (257, 32)\n",
      "to merge:   (97, 116)\n",
      "my bpe  [258, 99, 259, 32, 105, 115, 32, 105, 110, 32, 258, 104, 259]\n",
      "   lib: [257, 32, 99, 258, 259, 115, 259, 110, 32, 257, 32, 104, 258]\n",
      "vocab is  261\n",
      "to merge:   (116, 104)\n",
      "to merge:   (256, 101)\n",
      "to merge:   (257, 32)\n",
      "to merge:   (97, 116)\n",
      "to merge:   (32, 105)\n",
      "my bpe  [258, 99, 259, 260, 115, 260, 110, 32, 258, 104, 259]\n",
      "   lib: [257, 260, 258, 259, 115, 259, 110, 32, 257, 32, 104, 258]\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "num_merges = 5\n",
    "for i in range(1,num_merges+1):\n",
    "    vocab = 256 + i\n",
    "    print(\"vocab is \", vocab)\n",
    "    tokenizer = BPETokenizer()\n",
    "    tic = time.time()\n",
    "    tokenizer.train(sentence, vocab_size=vocab)\n",
    "    toc = time.time()\n",
    "    print(\"my bpe takes: \", toc-tic)\n",
    "    bpe = BytePairEncoder(num_merges=i)\n",
    "    print(\"my bpe \", bpe.encode(sentence=sentence))\n",
    "    print(\"   lib:\", tokenizer.encode(sentence))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "847393b4",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "cs336-llms-from-scratch",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
